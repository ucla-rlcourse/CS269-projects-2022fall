---
layout: post
comments: true
title: Meta Learning for Reinforcement Learning
author: John Arthur Dang and Felix Zhang (Team 01)
date: 2022-10-19
---


> For our project, we plan on applying MAML (as well as exploring newer versions such as iMAML) to more difficult RL task distributions than was used in the original MAML paper. Specifically we aim to investigate one and few-shot learning for various locomotion tasks of different creature types.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Related Papers

1. [*Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks*](https://arxiv.org/abs/1703.03400) [1]
This is the original MAML paper. It applies meta learning to locomotion gym tasks. The method directly optimize for meta parameters that are sensitive to changes in parameter space such that small gradient updates result in large changes in reward/loss in using gradients.


2. [*Meta-Learning with Implicit Gradients*](https://arxiv.org/abs/1909.04630) [2]

    MAML requires inner loop optimization, necessitating computation of Jacobian which is expensive. This paper proposes using implicit gradients, thereby calculates meta gradients using only final location of inner loop parameters rather than entire path


3. [*Probabilistic Model-Agnostic Meta-Learning*](https://arxiv.org/abs/1806.02817) [3]

    This paper covers the few shot learning setting, often not enough examples to fully specify task. It uses sample of model from model distribution to adapt to new task.


4. [*A Survey of Generalisation in Deep Reinforcement Learning*](https://arxiv.org/abs/2111.09794) [4]

    This paper covers current challenges zero shot policy transfer, especially out of distribution generalization where test and train distributions are different
5. [*MuJoCo Open AI Gym Environments*](https://www.gymlibrary.dev/environments/mujoco/) [5]
6. [*Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning*](https://meta-world.github.io/) [6]
7. [*D4RL: Datasets for Deep Data-Driven Reinforcement Learning* ](https://github.com/Farama-Foundation/D4RL)[7]

## MAML Overview & Approach

Model Agnostic Meta Learning proposed by Finn 2017 is a method for learning meta-parameters for fast adaptation to new tasks. For a given task distribution, MAML learns parameters that are sensitive to changes such that small changes in the parameter space result in large changes in the loss function or reward. 

![mamlgraphic]({{ '/assets/images/team01/mamlgraphic.png' | relative_url }})
{: style="width: 400px; max-width: 100%;text-align: center;"}
*Figure from MAML.* [1].
These parameters are found by a nested optimization loop. In the outer loop, a distribution of tasks is sampled. For each task, K gradient steps are taken to optimize parameters (initialized using the meta parameters) for each task with respect to each taskâ€™s loss function using gradient descent. The parameters computed to optimize each task are the adapted parameters. In the outer loop, the meta parameters are updated using the gradient of the meta parameters with respect to the sum of the losses of all tasks evaluated using the adapted parameters with gradient descent. Pseudocode for the MAML algorithm is shown below. In the original paper, MAML is shown to achieve state of that art or better performance on few shot learning tasks in vision and RL.


![MamlAlgo]({{ '/assets/images/team01/MamlAlgo.png' | relative_url }})
{: style="width: 400px; max-width: 100%; text-align: center;"}
*MAML Algorithm* [1].

## Proposal

For our project, we plan on applying MAML (and newer version of it like iMAML) to more difficult RL task distributions than was used in the original MAML paper. The RL task distributions in the original MAML paper were relatively simple: simulated mujoco robots were tasked with moving at a velocity sampled uniformly from a set range of velocities. We propose to use MAML to learn locomotion policies that are agnostic to the body type of the robot. In this case the task distribution would be motion in Ant, Cheetah, Humanoid and other robot body types. This task distribution is considerably more difficult than simply moving at different velocities with the same body. We hypothesize that MAML could learn parameters that are universal to locomotion more generally, including representations of physics dynamics and how leg movement affects robot motion

## Datasets

MujuCo is a dataset with locomotion 2-D tasks. It can be accessed through the openAI API and consists of state spacesof body parts and joints with corresponding velocities [5]. 


![MujuCo]({{ '/assets/images/team01/MujuCo.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*MujuCo Dataset: 2D Locomotion* [5].

If we have enough time, we will also investigate performance of MAML algorithms on the Meta-World and D4RL environments. Meta-World is a meta reinforcement learning benchmark that includes  a diverse set of different robotics arm manipulation tasks. 


![MetaWorld]({{ '/assets/images/team01/MetaWorld.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*MetaWorld Dataset* [6].

D4RL is a collection of datasets for offline reinforcement learning tasks. The original MAML paper evaluated MAML on policy gradient which is an online RL algorithm. It would be interesting to evaluate MAML performance on offline RL algorithms.

![D4RL]({{ '/assets/images/team01/D4RL.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*D4RL Dataset* [7].

## References

[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. "Model-agnostic meta-learning for fast adaptation of deep networks." International conference on machine learning. PMLR, 2017.

[2] Rajeswaran, Aravind, et al. "Meta-learning with implicit gradients." Advances in neural information processing systems 32 (2019).

[3] Finn, Chelsea, Kelvin Xu, and Sergey Levine. "Probabilistic model-agnostic meta-learning." Advances in neural information processing systems 31 (2018).

[4] Kirk, Robert, et al. "A survey of generalisation in deep reinforcement learning." arXiv preprint arXiv:2111.09794 (2021).

[5] Todorov, Emanuel, Tom Erez, and Yuval Tassa. "Mujoco: A physics engine for model-based control." 2012 IEEE/RSJ international conference on intelligent robots and systems. IEEE, 2012.

[6] Yu, Tianhe, et al. "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning." Conference on robot learning. PMLR, 2020.

[7] Fu, Justin, et al. "D4rl: Datasets for deep data-driven reinforcement learning." arXiv preprint arXiv:2004.07219 (2020).

---

