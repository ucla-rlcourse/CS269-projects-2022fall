---
layout: post
comments: true
title: Speed up Autonomous Driving with Safe Reinforce Learning in Metadrive
author: Yanxun Li & Zixian Li (Team 19)
date: 2022-10-15
---

> In MetaDrive, safety is the first priority. We want to explore how to speed up auto-drive agents with Safe Reinforcement Learning and find the optimal speed in diverse driving scenarios.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Abstract
 Reinforcement Learning (RL) is widely used in safe autonomous driving. MetaDrive [1] is a driving simulation platform that composes diverse driving scenarios for generalizable RL. In MetaDrive, safety is the first priority, where agents were trained to ensure the driving safety in diverse driving scenarios. However, safety is overemphasized at the expense of speed. For example, when there is traffic, an agent may keep the speed as low as possible and not overtake other vehicles. Imagine there is a busy driver who wants to pass through the traffic flow and arrive the destination in the shortest amount of time with safe driving. How can we achieve that in MetaDrive? So we are going to explore how RL can have agents achieve fastest speed with safety guaranteed.

## Objective
Explore how to speed up auto-drive agents with safe RL and find the optimal speed in diverse driving scenarios.

## Potential Approach
Under the MetaDrive environment, we will look into the reward function and adjust the reward for speed. The new reward function will reward more for higher speed than the old one. However, higher speed leads to lower safety. So we need to experiment to find out the balance between speed and safety.

In terms of maximizing speed, [2] and [3] used deep RL to optimaze car racing. Although speeding up our agents is far from racing, we can research into the two papers and leverage deep RL to optimize speed in our case.

With speed being considered as priority, the definition of safety needs to be changed slightly. In terms of safety, our goal is to avoid collisions between vehicles and achieve safe driving. For example, when there is no traffic light in the crossroad, the vehicle should not make dangerous decision to disturb other vehicles. There are two methods to achieve: 
- Increase the safety distance between vehicle and complete turning to its goal direction in a specific times.
- Minimize the disturbance to other vehicles by changing to other lanes with less cars and complete turning to its goal direction in a specific times.

We found [4] and [5] are helpful to reference in developing safe RL methods. We will firstly generate a few scenarios to simulate different traffic flows in MetaDrive from relatively empty road cases to busy road cases. Then a single agent will be designed and trained in those scenarios. We will focus on researching small sized vehicles and design the reward function mentioned above, where speed and risk factor are primary. After that, the designed agent could be trained by calculating its own reward on each state and update the state-value and policy using dynamic programming based on Markov decision process. Besides, we will compare both the policy iteration as well as the value iteration to see which will converges faster and which will generate the safest and fastest trajectary on the road.Chaning lane and overtaking is permitted to achieve faster speed.


## Potential Environment
MetaDrive in Python

## Relevant Papers
[1] Li, Quanyi, et al. "MetaDrive: Composing diverse driving scenarios for generalizable reinforcement learning." IEEE transactions on pattern analysis and machine intelligence (2022).

[2] Aldape, Pablo, and Samuel Sowell. "Reinforcement Learning for a Simple Racing Game." Stanford, 2018.

[3] Remonda, Adrian, et al. "Formula RL: Deep Reinforcement Learning for Autonomous Racing using Telemetry Data." CoRR, abs/2104.11106, 2021.

[4] D. Isele, A. Nakhaei and K. Fujimura, "Safe Reinforcement Learning on Autonomous Vehicles," 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018, pp. 1-6, doi: 10.1109/IROS.2018.8593420.

[5] Berkenkamp F, Turchetta M, Schoellig A, et al. Safe model-based reinforcement learning with stability guarantees[J]. Advances in neural information processing systems, 2017, 30.