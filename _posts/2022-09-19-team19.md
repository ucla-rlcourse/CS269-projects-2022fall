---
layout: post
comments: true
title: Speed up Autonomous Driving with Safe Reinforcement Learning in Metadrive
author: Yanxun Li & Zixian Li (Team 19)
date: 2022-10-15
---

> In MetaDrive, safety is the first priority. We want to explore how to speed up auto-drive agents with Safe Reinforcement Learning and find the optimal speed in diverse driving scenarios.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Abstract
 Reinforcement Learning (RL) is widely used in safe autonomous driving. MetaDrive [1] is a driving simulation platform that composes diverse driving scenarios for generalizable RL. In MetaDrive, safety is the first priority, where agents were trained to ensure the driving safety in diverse driving scenarios. However, safety is overemphasized at the expense of speed. For example, when there is traffic, an agent may keep the speed as low as possible and not overtake other vehicles. Imagine there is a busy driver who wants to pass through the traffic flow and arrive the destination in the shortest amount of time with safe driving. How can we achieve that in MetaDrive? So we are going to explore how RL can have agents achieve fastest speed with safety guaranteed.

## Objective
Explore how to speed up auto-drive agents with safe RL and find the optimal speed in diverse driving scenarios.

## Potential Approach
Under the MetaDrive environment, we will look into the reward function and adjust the reward for speed. The new reward function will reward more for higher speed than the old one. However, higher speed leads to lower safety. So we need to experiment to find out the balance between speed and safety.

In terms of maximizing speed, [2] and [3] used deep RL to optimaze car racing. Although speeding up our agents is far from racing, we can research into the two papers and leverage deep RL to optimize speed in our case.

With speed being considered as priority, the definition of safety needs to be changed slightly. In terms of safety, our goal is to avoid collisions between vehicles and achieve safe driving. For example, when there is no traffic light in the crossroad, the vehicle should not make dangerous decision to disturb other vehicles. There are two methods to achieve: 
- Increase the safety distance between vehicle and complete turning to its goal direction in a specific times.
- Minimize the disturbance to other vehicles by changing to other lanes with less cars and complete turning to its goal direction in a specific times.

We found [4] and [5] are helpful to reference in developing safe RL methods. We will firstly generate a few scenarios to simulate different traffic flows in MetaDrive from relatively empty road cases to busy road cases. Then a single agent will be designed and trained in those scenarios. We will focus on researching small sized vehicles and design the reward function mentioned above, where speed and risk factor are primary. After that, the designed agent could be trained by calculating its own reward on each state and update the state-value and policy using dynamic programming based on Markov decision process. Besides, we will compare both the policy iteration as well as the value iteration to see which will converges faster and which will generate the safest and fastest trajectary on the road.Chaning lane and overtaking is permitted to achieve faster speed.


## Potential Environment
MetaDrive in Python

## Midterm Progress
We have successfully set up the environment for MetaDrive. We dived into the source code of MetaDrive and figured out the architecture of MetaDrive e.g. the MataDrive Environment controlls agent vehicle with pre-trained PPO policy. 

We updated reward function in order to obtain an agent with faster speed and relatively high safety.

$$
R=c_1R_{driving}+c_2R_{speed}+R_{termination}
$$

We increased the maximum speed limitation of vehicle. At the same time, we increased the speed reward factor $$c_2$$ to emphasize the importance of speed, and thus the vehicle will be encouraged to take more acceleration as long as the safety factor is satisfied. Moreover, we set "use_lateral_reward" to false so that the vehicle will not keep in lane and learn to overtake cars and achieve faster speed. The newly updated reward function is used to in training process to generate the policy function.

We found MetaDrive uses RLLib to train RL agents. Ray 2.1.0 in RLLib is used to train and generate policy parameters with the PPO reinforcement training method under 22 environments for a large amount of episode, and the parameters are saved in the local folder. We also found MetaDrive uses expert class to read and make use of the trained data (e.g. weight), which returns the vehicle's current observation and continuous action to AI policy function. The trained policy function fits the fast and safe targets in the environment.

#### Next Steps: ####

We are going to research on how to run the training with RLLib and train the agent with the new reward function to see the result. We believe that the new reward function might not be sufficient to perfectly speed up the AI agent with safety, but it will serve as a good indication of what steps we need to take next e.g. change more parameters or change to a different RL method rather than PPO.


## Relevant Papers
[1] Li, Quanyi, et al. "MetaDrive: Composing diverse driving scenarios for generalizable reinforcement learning." IEEE transactions on pattern analysis and machine intelligence (2022).

[2] Aldape, Pablo, and Samuel Sowell. "Reinforcement Learning for a Simple Racing Game." Stanford, 2018.

[3] Remonda, Adrian, et al. "Formula RL: Deep Reinforcement Learning for Autonomous Racing using Telemetry Data." CoRR, abs/2104.11106, 2021.

[4] D. Isele, A. Nakhaei and K. Fujimura, "Safe Reinforcement Learning on Autonomous Vehicles," 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018, pp. 1-6, doi: 10.1109/IROS.2018.8593420.

[5] Berkenkamp F, Turchetta M, Schoellig A, et al. Safe model-based reinforcement learning with stability guarantees[J]. Advances in neural information processing systems, 2017, 30.
