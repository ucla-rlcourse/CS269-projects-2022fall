---
layout: post
comments: true
title: Optimization sum-up time-consuming for multi-agents
author: Liping Yin (Team 24)
date: 2022-11-13
---

> "How long will agents stay on the highway?" is one of the curious questions in autonomous driving that agents care about. Suppose those multi-agents will drive on and off-highway different freeway ramps, the project goal is to find an optimal way that all of the agents could sum up with the least time spent on the highway.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}


## Abstract
Reinforcement learning is a powerful data-driven control method in autonomous driving tasks. The MetaDrive simulator provides diverse driving scenarios for agents driving on the highway. "How long will agents stay on the highway?" is one of the curious questions that agents care about. Suppose those multi-agents will drive on and off-highway different freeway ramps, the project goal is to find an optimal way that all of the agents could sum up with the least time spent on the highway.

## Objective
Optimize the least sum-up time for all these multi-agents.

## Potential Approach
In this project, we also need to introduce a timer parameter into the action-value function. Despite the safety consideration, we also want to minimize the time spent.

Since it is a multi-agent problem, we should look into each agent first. For each agent, we might use the Q-learning algorithm to learn decision policies determined by the stateaction-value function Q, and Q estimates long-term discounted rewards for each state-action pair. For each action agent takes, we are considering long-term rewards since we can only know if those actions can maximize future rewards (until all the agents are off the highway).

We might proceed Temporal-Difference(TD) Learning to learn the utility of being in the states themselves. The advantage of TD could learn online after each step and learn from incomplete episodes. It is time-consuming if multi-agents take action and enumerates all possible outcomes. TD learning could bring a more efficient estimation of the value for each step.

## Potential Environment
MetaDrive with multi-agents at the highway

## 1. Introduction
Reinforcement learning utilizes a self-driven learning procedure that only requires a reward calculation based on the generated outputs. Once a generated sequence is feasible and its reward is derived, the desired meta-algorithm can be learned. This approach provides a general framework for optimizing decisions in dynamic environments, which can help to solve combinatorial optimization problems.

## 2. Multi-agent Model
(1) Time-consuming
We wish to minimize the smallest sum-up time-consuming. Disregarding different reinforcement learning methods, this calculation can be done in a new function that starts timing when the car runs and ends when cars reach the end (starting point are different but end places are the same).

(2) Store paths
Each agent can have a sequence for path that can avoid the collision. When a sub-tour has been constructed, the problem at that time is to find a path from the last agent for each vehicleâ€™s sub-tour through all unvisited agents to the depot. At that time, the requests of another agent already visited are irrelevant to the decision-making.

## 3. Current Approach
Google OR-Tools might benefit us from this optimization procedure. We need to try the instance with different numbers of agents and do the offline training and train the model with randomly generated data under some learning rate.

## 4. Next Steps
These are the algorithms we plan on building and testing:
- Deep Q Learning: learn decision policies determined by the stateaction-value function Q, and Q estimates long-term discounted rewards for each state-action pair.
- Temporal-Difference(TD): learn the utility of being in the states.
- Proximal Policy Optimization (with and without clipping): find optimal solutions for multiple agents interacting with each other.

## Relevant Papers
[1] Panait, L., & Luke, S. (2005). Cooperative multi-agent learning: The state of the art. Autonomous Agents and Multi-Agent Systems, 11(3), 387-434. doi:10.1007/s10458-005-2631-2
[2] Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. Cooperative Agents. Machine Learning Proceedings 1993, 330-337. doi:10.1016/b978-1-55860-307-3.50049-6
[3] Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., & Wu, Y. (2022, November 4). The surprising effectiveness of PPO in Cooperative, multi-agent games. arXiv.org. Retrieved November 13, 2022, from https://arxiv.org/abs/2103.01955
[4] Zemzem, W., & Tagina, M. (2017). Multi-agent coordination using reinforcement learning with a relay agent. Proceedings of the 19th International Conference on Enterprise Information Systems. doi:10.5220/0006327305370545
