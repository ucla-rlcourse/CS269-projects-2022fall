---
layout: post
comments: true
title: Human-in-the-Loop learning for AI pair programming
author: Rohan Wadhawan, Simran Masand, Subham Agrawal (Team 04)
date: 2022-10-14
---

>  Human-in-the-Loop learning for AI pair programming (PZH: Detailed summary needed!)



<!--more-->
{: class="table-of-content"}
* TOC
{:toc}


# Good Cop, Bad Cop

## Team Members
- Rohan Wadhawan (905711905)
- Simran Masand (706085004)
- Subham Agrawal (205948930)

<br>

## Motivation


A fundamental way to approach AI is to question whethere we are teaching machines to mimic human outputs or to mimic human thinking. The Human-in-the-loop (HITL) approach focusses on the latter introducing human knowledge gleaned from partial demonstration into the proxy value function by Offline RL techniques. AI agent and human agent are working sumultaneously to obtain a common goal. <br>
Our paper explores the role of human as an "Adversary" and as an "Expert".



<br>

### Objectives:
<ol>
<li>Implementing vanilla-RL agent in Minigrid world travelling from start point to goal state. </li> 
<li>Introduce human adversary by adding dynamic obstacles to the minigrid-world interactively and evaluating performance.
</li>
<li>Applying HITL technique using HACO algorithm with human adversary in minigrid-world. </li>
</ol>

<!-- 
### Proposed Methodology:
<ol>
<li>Learning from demonstration via Offline RL: <br> 
For complex environments, it can be very difficult to summarised behaviors and skills sets to apply to a reward function. Instead, imitation learning sets a precedent via humanan generated state and action sequences.
  </li> 
<li>Intervention Minimization: <br>
 The human intervention and the partial demonstration are two sources of informative training data. The goal is to minimize human intervention due to the cost associated with human intervention. </li>
<li></li>
</ol> -->

<!-- <img src = "HITL_image.png" length=500><em>CC: https://decisionforce.github.io/HACO/</em> -->

<!-- ## Metrics of Success:
<ol><li>Test Success Rate </li> 
<li>Intervention Cost: Expected to reduce with steps </li>
<li>Takeover Rate: Expected to reduce with steps </li>
</ol> -->

## Potential Environments
- Minigrid
  - Usage: We will be implementing the vanilla RL. Using the Dynamic environment, we will introduce the human adversary and finally using HITL, introduce human expert and benchmark performances in each of those scenarios.
- MetaDrive:
  - Reproduce HACO to understand its optimization (Indicator function, Human Intervention Cost, Proxy-Value Function)
  
## Potential Benchmarks
- Baseline is vanilla RL
- Vanilla RL with Human Adversary
- Human-in-the-Loop with Adversary and Expert

## Relevant Papers
- Li, Q., Peng, Z., & Zhou, B. (2022). Efficient Learning of Safe Driving Policy via Human-AI Copilot Optimization. arXiv preprint arXiv:2202.10341.
- Reddy, S., Dragan,A., Levine, S. (2018) Shared Autonomy via Deep Reinforcement Learning
- Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., & Bengio, Y. (2019). BabyAI: First steps towards grounded language learning with a human in the loop. In International Conference on Learning Representations (Vol. 105).





