---
layout: post
comments: true
title: Off-policy Meta Reinforcement Learning in a Multi-Agent Competitive Environment
author: Christian Natajaya (Team 21)
date: 2022-12-8
---


## Abstract
 
In this project, we will investigate different applications of meta-RL. Previous investigations have been centered on using meta-RL to expedite single agents learning new unseen tasks drawn from the same task distribution. In this project, we will explore how meta-RL trained agents perform in a competitive multi-agent RL setting when having to play against new unseen agents. We will specifically implement the PEARL (off-policy meta-RL) algorithm developed by Kate Rakelly, et al. Instead of relying on LSTM recurrence in the policy network to memorize skills, PEARL decouples this from the policy by using a network to encode previous experiences into a latent context variable. This latent context variable is used to condition both actor (policy) and critic (value) architectures, We will use Derkgym as the multi-agent gym environment to test the PEARL algorithm. We will first train the agent against another agent that will only perform random actions. After collecting this experience, we will then expose the agent to new opponents with different skillsets. Using meta-RL, we anticipate that the agent will be able to adapt to new opponents quicker.


## Introduction

Conventional reinforcement learning algorithms aim to learn a policy that maximizes the trajectory rewards for a Markov decision process (**eq.1**). This learned policy generally does not translate over to new unseen tasks; a distribution shift in the observation space may cause the policy to perform suboptimal actions, and a change in the environment rewards and state transitions means the same actions may yield different trajectories. Therefore, conventional reinforcement learning algorithms need to learn a separate policy for each new task, makig it extremely sample inefficient. 

In contrast, meta reinforcement learning algorithms are able to learn new and unseen tasks more efficiently, much like how people can learn new skills quickly using previously learned skills. In meta reinforcement learning, the algorithm models a function that takes in a Markov decision process as input $f_{\theta}$ (**eq.3**). It then learns to optimize this function so that the policy maximizes the trajectory reward over any Markov decision process that is drawn from the task distribution. This can be accomplished by learning how to encode context from experience and using this encoding to condition a policy so that it universally applies to every task in the distribution. PEARL - an efficient off-policy meta reinforcement learning algorithm - operates on this basis as explained later in **Algorithm**.

**Reinforcement Learning Objective**

$$ (eq.1) \theta^* = argmax_{\theta} E_{\pi_{\theta}(\tau)}\[R(\tau)\] \\ $$ 

**Meta Reinforcement Learning Objective**

$$ (eq.2) \theta^* = argmax_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}\[R(\tau)\] \\ $$
 
$$ \phi_{i} = f_{\theta}(M_{i}) \\ $$

This project applied the PEARL algorithm to Derk's gym, a multi-agent competitive environment. In meta reinforcement learning, the train and test tasks are generally different from each other but are drawn from the same distribution. We create this problem by assigning train and test tasks that consist of agents with a random combination of abilities. During testing, the policy controls a combination of agents it has never played with and competes with a combination of agents it has never played against. This effectively changes the nature of every task while ensuring each task still belongs in the same distribution, recreating the problem that meta reinforcement learning aims to solve. 

We hypothesize that conventional reinforcement learning algorithms will struggle to learn a policy that is able to control and play against agents with  abilities that it has not seen before. With meta reinforcement learning, we hope that by exposing the algorithm to a variety of agents with different randomized abilities, it is able to learn to make inferences from context and use this to optimize a policy to solve the unseen task. In other words, the PEARL algorithm should be able to use its experience playing against an assortment of agents as context to create an action hypothesis when it plays against a combination of agents it has not seen before. The PEARL algorithm will then evaluate this hypothesis and learn to make better context inferences and policy outputs.


## Contribution

In this project, we used the PEARL algorithm written by Kate Rakelly, et al. as the basis of our code and changed the environment from MuJoCo -- a single agent environment -- to Derk's Gym -- a multi-agent evnrionment. In Derk's gym, there are 3 agents in the home team and 3 agents in the opposing team. For each agent, the observation space is 64; some of these include the abilities of the controlled agent, the abilities of the opposing agent, and agent positions on the map. For each agent, the action space is 5; some of these actions include rotating, moving, and using any of the three abilities. To facilitate multi-agent learning, we implement one shared policy (actor) to control the actions of all 3 agents, and one shared critic to evaluate the policy. These actor-critic networks will take in the states, actions, and latent variables of all 3 agents concatenated together.

<p align="center">
    <img width="300" src="https://github.com/ChrisNA97/CS269-projects-2022fall/blob/main/assets/images/team21/Derk%20Gym.png" alt="Derk's Gym Environment">
<p align="center">
 Fig 2. Derk's Gym Environment. 
</p>

In MuJoCo, the agent trained using the PEARL algorithm was able to successfully learn new tasks quicker than other RL methods. For example, a half cheetah trained to move in one velocity was able to efficiently learn to run in another target velocity during testing. We wish to explore whether this success translates over to multi-agent environments. During training, we will let the agents have different abilities and play against other agents (which are controlled by a random policy) with different abilities. During testing, we will provide the algorithm with a different combination ofabilities it has not seen before. We wish to explore if the inference network is able to encode strategies it learned through its experiences... It uses context to hypothesize how to play with certain characters, and how to play against certain characters....


## Algorithm

> Random Policy

The opposing agents in Derk's gym will be controlled using a random policy that simply samples the action space. This random policy will be deployed on a local Derk's gym server where we will train our baseline and PEARL algorithms. 

> Baseline Policy

The baseline algorithm used to represent conventional reinforcement learning, models the policy as a neural network that takes in the states as input and outputs the actions. We apply evolutionary strategy to train the neural network; at every episode, we perturb the neural network weights with a Gaussian noise and pick the weights that lead to the highest trajectory reward. 

> PEARL Policy

**Inference network (also refered to as context encoder)**

This is the main novelty of the PEARL algorithm. The PEARL algorithm retrieves experiences in the replay buffer for context, which it uses to hypothesize what actions to perform for the current task. This is done by passing the context (C) into an inference network $q(z\|c)$ to generate a probabilistic latent variable Z, which is effectively a posterior over the context. This probabilistic latent variable is then sampled and used to condition the policy given by $\pi_{\theta}(a|s, z)$. Essentially, the context is used to adapt the agents' policy behaviors to the new unseen task. The weights of the inference network are optimized jointly with the weights of the actor-critic networks using gradients from the critic update - which will be discussed further below.

**Soft actor critic reinforcement learning**

The PEARL algorithm is built on top of the soft actor-critic algorithm (SAC), which is an off-policy method that augments the discounted returns with the policy entropy. The critic consists of two q-value networks $Q_{\theta}(s,a,z)$ and one value network $(V_{\phi}(s,z))$, and the actor consists of one policy function $\pi_{\theta}(a \| s,z)$. As mentioned earlier, the novelty of PEARL is that these actor-critic networks also take in the latent variable returned by the inference network as input, so that they are conditioned by the context posterior. We optimize the parameters of the actor-critic networks using the ADAM optimizer and the loss functions below in **Update Rules**.

<p align="center">
    <img width="300" src="https://github.com/ChrisNA97/CS269-projects-2022fall/blob/main/assets/images/team21/MetaRL%20Loop.png" alt="PEARL Training Loop">
<p align="center">
 Fig 1. PEARL Algorithm: The inference network qÏ† uses context data to infer the posterior over the latent context variable Z, which conditions the actor and critic, and is optimized with gradients from the critic as well as from an information bottleneck on Z. 
</p>

> Update Rules

**Loss Functions**

$$ L_{V} = E_{s,a,r,s'\sim B, z\sim q_{\phi}(z|c)} \[V_{\phi}(s,z) - min_{i=1,2}Q_{\theta, i}(s',a',z) - log \pi_{\theta}(a'\|s')\] \\ $$

$$ L_{Q} = E_{s,a,r,s'\sim B, z\sim q_{\phi}(z|c)} \[Q_{\theta}(s,a,z) - (r+V_{\phi}(s',z))\]^2 \\ $$

$$ L_{\pi} = E_{s \sim B, a \sim \pi_{\theta}, z \sim q_{\phi}(z|c)} \left[D_{KL}\left(\pi_{\theta}(a|s,z) \|\| \frac{\exp{Q_{\theta}(s,a,z)}}{Z_\theta (s)}\right)\right] \\ $$

**Stochastic Gradient Descent**

$$ \phi_{V} \leftarrow \phi - \alpha \nabla_{\phi} \sum_{i} L_{V}^{i} \\ $$

$$ \theta_{Q} \leftarrow \theta_{Q} - \alpha \nabla_{\theta_{Q}} \sum_{i} L_{Q}^{i} \\ $$

$$ \theta_{\pi} \leftarrow \theta_{\pi} - \alpha \nabla_{\theta_{\pi}} \sum_{i} L_{\pi}^{i} \\ $$

$$ \phi_{q(z \| c} \leftarrow \phi - \alpha \nabla_{\phi} \sum_{i} (L_{Q}^{i} + L_{KL}^{i}) \\ $$


## Results

> Observations

The agents trained using the baseline algorithm learned to fight with a couple of characters very well (that it has learned previously). However when a new character is introduced, the agent ends up circling around the map and falling off the map. The agents trained using PEARL learned to focus on the enemy tower and the enemies. However, the agent hesitates to attack since they have different abilities and are unable to learn how to use them. Eventually leading to zero rewards.

> Average path reward 

The table below summarizes the average of trajectory rewards across all agents for a single game episode. This was retrieved during testing, when the policy was made to play with 3 agents with an unseen combination of abilities, against another 3 agents also with an unseen combination of abilities controlled by a random policy.

| Game | Baseline Reward | PEARL Reward | 
| ---- | --------------- | ------------ | 
|    1 |          100    |      0       |
|    2 |          -100   |     30       |
|    3 |          -100   |     106      |
|    4 |          -100   |      90      |
|    5 |           100   |      20      |
|    6 |           100   |     120      |
|    7 |          -100   |      18      |
|    8 |          -100   |      0       |
|    9 |           100   |      0       |
|    10|          -100   |      45      |
| Mean |          -10    |      43      |


> Future Improvements

Several improvements could be made to the environment. First, the environment returns a positive reward when an opposing agent dies or an opposing tower is destroyed, regardless of who is responsible for the action (even if the opposing agent is responsible for the action). This causes the algorithm to learn actions that do not necessarily lead to the rewards. Second, the environment does sufficient penalize agent actions that kill a teammate or destroy its own tower. The rewards could be made more negative so the policy is discouraged from performing such actions. Finally, the maximum step length of the algorithm could be increased since several episodes terminate before the agent has successfully killed an opposing agent or destroyed an opposing tower. This leads to some episodes with zero rewards even if the agent has started taking favorable actions, so the policy fails to learn these optimal behavior.


## Reference
[1] Derkgym documentation: http://docs.gym.derkgame.com/#examples
[2] Random agent in Derkgym: https://github.com/MountRouke/Randos
[3] PEARL code: https://github.com/katerakelly/oyster
[4] Meta reinforcement learning: https://metalearning-cvpr2019.github.io/assets/CVPR_2019_Metalearning_Tutorial_Chelsea_Finn.pdf
[5] PEARL paper: https://arxiv.org/pdf/1903.08254.pdf

[example] Dwibedi, Debidatta, et al. "Counting out time: Class agnostic video repetition counting in the wild." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.   

---
