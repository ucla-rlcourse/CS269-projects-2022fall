---
layout: post
comments: true
title: 2 vs 2 soccer game
author: Linqiao Jiang, Qi Li, Huizhuo Yuan (Team 05)
date: 2022-10-24
---


> In this project we are going to investiage **self-play for competitive Multi-agent Reinforcement Learning (MARL)**. We will explore the power of self-play on stablizing multi-agent training on several settings including soccer game, Multi-agent tennis, competitive Atari Game Pone, competitive Car-racing, etc. Among them, we will build a 2 vs 2 soccer game in Unity and try different MARL algorithms on it. We will provide the source code and detailed analysis.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}


---

## Project Proposal 

In this project we will investigate **self-play for competitive Multi-agent Reinforcement Learning (MARL)**. Different from single-agent systems, where a stationary environment is possible, the evolution of the environmental state and the reward function that each agent received are not only determined by the envrionment, but also other agents' joint actions. As a result, agents need to take into account and interact with not only the environment but also other intelligent agents (see Fig. 1). In a multi-agents scenario, there could be cooperation (e.g., May and Cody in video game *It Takes Two*) and competition (e.g., most of board games).

<figure align="center">
  <img width="90%" src="../../../assets/images/team05/MARL.png">
  <figcaption>Fig 1. Single-agent vs Multi-agent [1].</figcaption>
</figure>

Competitive multi-agent algorithms are widely applicable, with particular intest in a variety of board, sports, or computer games including Chess, Mahjong, Soccer, Starcraft, etc. However, training agents to perform complex tasks requires high complexity of the environment, which is in general hard to achieve in regular training environments. Self-play is a concept that can be dated back to TD-gammon and has been explored to fit real tasks in AlphaGo, Dota2, etc. In a self-play game, the opponent agent are providing sufficient diversified responses, which resolves the issue of the lack of complexity in the environment. Moreover, the opponent agent are providing the trained agent with the right curriculum to learn.

More specifically, in this post, our main target is to implement a toy 2 vs 2 soccer game in Unity (as shown in Fig. 2) and investigate the performance of self-play. In this game, there are two teams with two agents in each. The goal is to get the ball into the opponent's goal while preventing the ball from entering own goal.

<figure align="center">
  <img width="80%" src="../../../assets/images/team05/soccer.png">
  <figcaption>Fig 2. 2 vs 2 soccer game in Unity [2].</figcaption>
</figure>


To achive this, we are going to firstly build up the soccer environment in Unity [2]. Then we plan to create a 1 vs 1 soccer game to investigate the behaviors of competition. After the 1 vs 1 scenario works, we will handle the 2 vs 2 soccer game which involves cooperation too.


## Challenges and Solving the Problem

A challenge that we are facing in training the two-soccer game is that we need to train more than one brain at a time, e.g. in the soccer game setting, there is a goalie that needs a defensive brain, and a striker that needs an offensive brain. So we need different reward functions to train different controllers. By enabling the multi-brain training feature in the ML-Agents toolkit of Unity, we are building our algorithms based on not only a two-player setting, but also each player has two brains present. After training, we get one neural network model for each brain, that further enables mixing and matching different hyper parameters.

In the video, it shows an example of a trained two-player two-brain agents on soccer game. During the training, the goalie’s reward is given such that it losses score when the ball is in the gate and the striker’s reward is set such that it gains score 1 when the ball gets in the other side. Thus, we can observe different behaviors lead by different brains. For example, the goalie always stays around the gate, it moves fast towards the preceding direction of the ball when the ball is near the gate, and stays mostly still when the ball is far away. On the other hand, the behavior of the striker is more complicated and is more challenging to train, the striker interacts frequently with both the goalie and the striker of the other side.

Another challenge we are facing is alongside the cooperation of the two brains, how to train the adversarial between two-players, we propose to experiment on using the self-play technique to enhance the performance. We will analyze how the two-brain setting affects the training, as well as how the self-play affects the training.

## Relavant Papers and Methodology

We plan to implement some of the below **MARL algorithms** [3] and analyse their behaivors:

- Self-play
  - [Emergent complexity via multi-agent competition](https://arxiv.org/pdf/1710.03748.pdf) [15]

- Independent Learning
  - [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/1803.11485.pdf) [4]
- Value Decomposition
  - [VDN：Value-Decomposition Networks For Cooperative Multi-Agent Learning](https://arxiv.org/pdf/1706.05296) [5]
  - [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf) [6]
  - [QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1905.05408) [7]
- Policy Gradient
  - [COMA：Counterfactual Multi-Agent Policy Gradients](https://arxiv.org/abs/1705.08926) [8]
  - [MADDPG：Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf&quot;&gt;Multi-Agent) [9]
- Communication
  - [BiCNet：Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games](https://arxiv.org/abs/1703.10069) [10]
  - [CommNet：Learning Multiagent Communication with Backpropagation](https://arxiv.org/abs/1605.07736) [11]
  - [IC3Net：Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks](https://arxiv.org/abs/1812.09755) [12]
  - [RIAL/RIDL：Learning to Communicate with Deep Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1605.06676) [13]
- Exploration
  - [MAVEN：Multi-Agent Variational Exploration](https://arxiv.org/pdf/1910.07483) [14]




Hopefully, we can build a game environment like this:

<p align="center">
	<iframe width="618" height="473" src="https://www.youtube.com/embed/Hg3nmYD3DjQ" frameborder="0" allowfullscreen ng-show="showvideo"></iframe>
</p>


Specicfically, two team contains both a *striker* and a *goalie*, and each trained using separate reward functions and brains. Firstly, We set up the trainer as follows:


| Setting         | Detail                        | 
| --------------- | ----------------------------------------------------------- |
| Striker Goal    | Get the ball into the oppoents goal                         | 
| Goalie Goal     | Defend own goal from oppoents                               |
| Observations    | Local ray-case perception on nearby objects                 |
| Actions         | Movement and rotation in x, z plane                         |
| Striker Rewards | +1 when its team scores goal; -0.1 when oppoent scores goal | 
| Goalie Rewards  | -1 when oppoent scores goal; +0.1 when its team scores goal |

## Conclusion


We will also create a GUI to allow users to flight with RL agents, if time allows.

It is possible that some algorithms don't work well in the soccer settings. We will try to finetune the hyper-parameters and analyse why some of the algorithms fail.

By this project, we hope to provide a easy-to-use multi-agent environment and plug-and-play MARL algorithms to class and, also give detailed analysis of behaviors of MARL agents to readers.


## Other Potential Environments
- [Multi-agent tennis](https://github.com/kantologist/multiagent-sac)
- [Competitive pone](https://github.com/ucla-rlcourse/competitive-rl)
- [Competitive car-racing](https://github.com/ucla-rlcourse/competitive-rl)


## Implementation

### Environment setup

Follow the [instruction](https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/Installation.md) to setup environment. Specifically, you need to:

1. Install Unity 2021.3.11f1 from [Unity Hub](https://unity3d.com/get-unity/download).
2. Clone the [repo](https://github.com/liqi0126/unity_soccer). 
3. Install `com.unity.ml-agents` and `com.unity.ml-agents.extensions` in Unity.
4. Install `ml-agents` and `torch` in conda environment.

### MARL Training

1. Download [SoccerTwo.app](https://web.cs.ucla.edu/~liqi0126/files/SoccerTwo.app.tar) and uncompress. You can also follow the instruction from [here](https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/Learning-Environment-Executable.md) to build SoccerTwo.app by yourself.
2. In the github repo, run

``` 
mlagents-learn config/poca/SoccerTwos.yaml  --env=SoccerTwo --run-id=[job id]
```

3. You can edit `SoccerTwos.yaml` to try new algorithms
4. You can monitor the training process in tensorboard with `tensorboard --logdir results`.

### Testing

1. The SoccerTwos.onnx in your `results/[your job id]/` folder is the checkpoint for you MARL algorithm.
2. Follow the step [here](https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/Getting-Started.md#embedding-the-model-into-the-unity-environment) to test your model performance in Unity environmnt.

## Results

### POCA

**Introduction:** 

In this project, we first provide [MA-POCA](https://arxiv.org/pdf/2111.05992.pdf) (MultiAgent POsthumous Credit Assignment), a neural network that acts as a "coach" for a whole group of agents. Since Muti-Agent game needs to be considered with functionality for training cooperative behaviors - i.e., groups of agents working towards a common goal, where the success of the individual is linked to the success of the whole group. In such a scenario, agents typically receive rewards as a group. You can give rewards to the team as a whole, and the agents will learn how best to contribute to achieving that reward. Agents can also be given rewards individually, and the team will work together to help the individual achieve those goals. During an episode, agents can be added or removed from the group, such as when agents spawn or die in a game. If agents are removed mid-episode (e.g., if teammates die or are removed from the game), they will still learn whether their actions contributed to the team winning later, enabling agents to take group-beneficial actions even if they result in the individual being removed from the game (i.e., self-sacrifice). MA-POCA can also be combined with self-play to train teams of agents to play against each other.

**Traning results:**
In the next two fitures we present our training curve of the POCA algorithm on the two-player soccer game environment. Figure 3 shows the training error during training and Figure 4 shows the group cumulative rewards across the training process.


<figure align="center">
  <img width="80%" src="../../../assets/images/team05/poca_ep_len.png">
  <figcaption>Fig 3. episode length.</figcaption>
</figure>



<figure align="center">
  <img width="80%" src="../../../assets/images/team05/poca_group_cumulative_r.png">
  <figcaption>Fig 4. group cumulative reward.</figcaption>
</figure>

Finally, in this video, we demonstrate the behavior of our trained agent. We see that it follows similar behavior as the example video above. The goalie stays around the gate to defend when the ball gets close to the gate, and the striker moves intensively across the field. However, we notice that the role of the goalie and the striker is switching during an episode, meaning that sometimes the goalie moves forward to shoot. We anticipate that this is due to improper assignment of goals for the defenser and the attacker. We will try solving this issue and improving the performance in the upcoming weeks.


<p align="center">
	<iframe width="618" height="473" src="https://www.youtube.com/embed/m-LZjAXWJ5w" frameborder="0" allowfullscreen ng-show="showvideo"></iframe>
</p>



## References

1. Yang, Yaodong, and Jun Wang. “An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective.” ArXiv.org, 18 Mar. 2021, https://arxiv.org/abs/2011.00583. 
2. https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos
3. https://github.com/TimeBreaker/MARL-papers-with-code/blob/main/README.md
4. Rashid, Tabish, et al. “QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.” ArXiv.org, 6 June 2018, https://arxiv.org/abs/1803.11485. 
5. Sunehag, Peter, et al. “Value-Decomposition Networks for Cooperative Multi-Agent Learning.” ArXiv.org, 16 June 2017, https://arxiv.org/abs/1706.05296v1. 
6. Rashid, Tabish, et al. "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning." 2018, http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf. 
7. Son, Kyunghwan, et al. “QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning.” ArXiv.org, 14 May 2019, https://arxiv.org/abs/1905.05408. 
8. Foerster, Jakob, et al. “Counterfactual Multi-Agent Policy Gradients.” ArXiv.org, 14 Dec. 2017, https://arxiv.org/abs/1705.08926. 
9. Lowe, Ryan, et al. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.” ArXiv.org, 14 Mar. 2020, https://arxiv.org/abs/1706.02275. 
10. Peng, Peng, et al. “Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-Level Coordination in Learning to Play Starcraft Combat Games.” ArXiv.org, 14 Sept. 2017, https://arxiv.org/abs/1703.10069. 
11. Sukhbaatar, Sainbayar, et al. “Learning Multiagent Communication with Backpropagation.” ArXiv.org, 31 Oct. 2016, https://arxiv.org/abs/1605.07736. 
12. Singh, Amanpreet, et al. “Learning When to Communicate at Scale in Multiagent Cooperative and Competitive Tasks.” ArXiv.org, 23 Dec. 2018, https://arxiv.org/abs/1812.09755. 
13. Foerster, Jakob N., et al. “Learning to Communicate with Deep Multi-Agent Reinforcement Learning.” ArXiv.org, 24 May 2016, https://arxiv.org/abs/1605.06676. 
14. Mahajan, Anuj, et al. “Maven: Multi-Agent Variational Exploration.” ArXiv.org, 20 Jan. 2020, https://arxiv.org/abs/1910.07483. 
15. Bansal T, Pachocki J, Sidor S, et al. Emergent complexity via multi-agent competition[J]. arXiv preprint arXiv:1710.03748, 2017.
