---
layout: post
comments: true
title: 2 vs 2 Soccer Game
author: Linqiao Jiang, Qi Li, Huizhuo Yuan (Team 05)
date: 2022-10-24
---


> In this project we are going to investiage **self-play for competitive Multi-agent Reinforcement Learning (MARL)**. We will explore the power of self-play on stablizing multi-agent training on several settings including soccer game, Multi-agent tennis, competitive Atari Game Pone, competitive Car-racing, etc. Among them, we will build a 2 vs 2 soccer game in Unity and try different MARL algorithms on it. We will provide the source code and detailed analysis.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

---

## Project Proposal 

In this project we will investigate **self-play for competitive Multi-agent Reinforcement Learning (MARL)**. Different from single-agent systems, where a stationary environment is possible, the evolution of the environmental state and the reward function that each agent received are not only determined by the envrionment, but also other agents' joint actions. As a result, agents need to take into account and interact with not only the environment but also other intelligent agents (see Fig. 1). In a multi-agents scenario, there could be cooperation (e.g., May and Cody in video game *It Takes Two*) and competition (e.g., most of board games).

<figure align="center">
  <img width="90%" src="../../../assets/images/team05/MARL.png">
  <figcaption>Fig 1. Single-agent vs Multi-agent [1].</figcaption>
</figure>

Competitive multi-agent algorithms are widely applicable, with particular intest in a variety of board, sports, or computer games including Chess, Mahjong, Soccer, Starcraft, etc. However, training agents to perform complex tasks requires high complexity of the environment, which is in general hard to achieve in regular training environments. Self-play is a concept that can be dated back to TD-gammon and has been explored to fit real tasks in AlphaGo, Dota2, etc. In a self-play game, the opponent agent are providing sufficient diversified responses, which resolves the issue of the lack of complexity in the environment. Moreover, the opponent agent are providing the trained agent with the right curriculum to learn.

More specifically, in this post, our main target is to implement a toy 2 vs 2 soccer game in Unity (as shown in Fig. 2) and investigate the performance of self-play. In this game, there are two teams with two agents in each. The goal is to get the ball into the opponent's goal while preventing the ball from entering own goal.

<figure align="center">
  <img width="80%" src="../../../assets/images/team05/soccer.png">
  <figcaption>Fig 2. 2 vs 2 soccer game in Unity [2].</figcaption>
</figure>


To achive this, we are going to firstly build up the soccer environment in Unity [2]. Then we plan to create a 1 vs 1 soccer game to investigate the behaviors of competition. After the 1 vs 1 scenario works, we will handle the 2 vs 2 soccer game which involves cooperation too.

## Relavant Papers and Methodology

We plan to implement some of the below **MARL algorithms** [3] and analyse their behaivors:

- Self-play
  - [Emergent complexity via multi-agent competition](https://arxiv.org/pdf/1710.03748.pdf) [15]

- Independent Learning
  - [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/1803.11485.pdf) [4]
- Value Decomposition
  - [VDN：Value-Decomposition Networks For Cooperative Multi-Agent Learning](https://arxiv.org/pdf/1706.05296) [5]
  - [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf) [6]
  - [QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1905.05408) [7]
- Policy Gradient
  - [COMA：Counterfactual Multi-Agent Policy Gradients](https://arxiv.org/abs/1705.08926) [8]
  - [MADDPG：Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf&quot;&gt;Multi-Agent) [9]
- Communication
  - [BiCNet：Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games](https://arxiv.org/abs/1703.10069) [10]
  - [CommNet：Learning Multiagent Communication with Backpropagation](https://arxiv.org/abs/1605.07736) [11]
  - [IC3Net：Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks](https://arxiv.org/abs/1812.09755) [12]
  - [RIAL/RIDL：Learning to Communicate with Deep Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1605.06676) [13]
- Exploration
  - [MAVEN：Multi-Agent Variational Exploration](https://arxiv.org/pdf/1910.07483) [14]




Hopefully, we can build a game environment like this:

<p align="center">
	<iframe width="618" height="473" src="https://www.youtube.com/embed/Hg3nmYD3DjQ" frameborder="0" allowfullscreen ng-show="showvideo"></iframe>
</p>

We will also create a GUI to allow users to flight with RL agents, if time allows.

It is possible that some algorithms don't work well in the soccer settings. We will try to finetune the hyper-parameters and analyse why some of the algorithms fail.

By this project, we hope to provide a easy-to-use multi-agent environment and plug-and-play MARL algorithms to class and, also give detailed analysis of behaviors of MARL agents to readers.



## Other Potential Environments
- [Multi-agent tennis](https://github.com/kantologist/multiagent-sac)
- [Competitive pone](https://github.com/ucla-rlcourse/competitive-rl)
- [Competitive car-racing](https://github.com/ucla-rlcourse/competitive-rl)

## References

1. Yang, Yaodong, and Jun Wang. “An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective.” ArXiv.org, 18 Mar. 2021, https://arxiv.org/abs/2011.00583. 
2. https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos
3. https://github.com/TimeBreaker/MARL-papers-with-code/blob/main/README.md
4. Rashid, Tabish, et al. “QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.” ArXiv.org, 6 June 2018, https://arxiv.org/abs/1803.11485. 
5. Sunehag, Peter, et al. “Value-Decomposition Networks for Cooperative Multi-Agent Learning.” ArXiv.org, 16 June 2017, https://arxiv.org/abs/1706.05296v1. 
6. Rashid, Tabish, et al. "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning." 2018, http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf. 
7. Son, Kyunghwan, et al. “QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning.” ArXiv.org, 14 May 2019, https://arxiv.org/abs/1905.05408. 
8. Foerster, Jakob, et al. “Counterfactual Multi-Agent Policy Gradients.” ArXiv.org, 14 Dec. 2017, https://arxiv.org/abs/1705.08926. 
9. Lowe, Ryan, et al. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.” ArXiv.org, 14 Mar. 2020, https://arxiv.org/abs/1706.02275. 
10. Peng, Peng, et al. “Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-Level Coordination in Learning to Play Starcraft Combat Games.” ArXiv.org, 14 Sept. 2017, https://arxiv.org/abs/1703.10069. 
11. Sukhbaatar, Sainbayar, et al. “Learning Multiagent Communication with Backpropagation.” ArXiv.org, 31 Oct. 2016, https://arxiv.org/abs/1605.07736. 
12. Singh, Amanpreet, et al. “Learning When to Communicate at Scale in Multiagent Cooperative and Competitive Tasks.” ArXiv.org, 23 Dec. 2018, https://arxiv.org/abs/1812.09755. 
13. Foerster, Jakob N., et al. “Learning to Communicate with Deep Multi-Agent Reinforcement Learning.” ArXiv.org, 24 May 2016, https://arxiv.org/abs/1605.06676. 
14. Mahajan, Anuj, et al. “Maven: Multi-Agent Variational Exploration.” ArXiv.org, 20 Jan. 2020, https://arxiv.org/abs/1910.07483. 
15. Bansal T, Pachocki J, Sidor S, et al. Emergent complexity via multi-agent competition[J]. arXiv preprint arXiv:1710.03748, 2017.
