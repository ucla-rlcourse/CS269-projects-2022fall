---
layout: post
comments: true
title: 2 vs 2 soccer game
author: Linqiao Jiang, Qi Li, Huizhuo Yuan (Team 05)
date: 2022-10-24
---


> In this project we are going to investiage **self-play for competitive Multi-agent Reinforcement Learning (MARL)**. We will explore the power of self-play on stablizing multi-agent training on several settings including soccer game, Multi-agent tennis, competitive Atari Game Pone, competitive Car-racing, etc. Among them, we will build a 2 vs 2 soccer game in Unity and try different MARL algorithms on it. We will provide the source code and detailed analysis.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Project Proposal 

In this project we will investigate **self-play for competitive Multi-agent Reinforcement Learning (MARL)**. Different from single-agent systems, where a stationary environment is possible, the evolution of the environmental state and the reward function that each agent received are not only determined by the envrionment, but also other agents' joint actions. As a result, agents need to take into account and interact with not only the environment but also other intelligent agents (see Fig. 1). In a multi-agents scenario, there could be cooperation (e.g., May and Cody in video game *It Takes Two*) and competition (e.g., most of board games).

<figure align="center">
  <img width="90%" src="../../../assets/images/team05/MARL.png">
  <figcaption>Fig 1. Single-agent vs Multi-agent [1].</figcaption>
</figure>

Competitive multi-agent algorithms are widely applicable, with particular intest in a variety of board, sports, or computer games including Chess, Mahjong, Soccer, Starcraft, etc. However, training agents to perform complex tasks requires high complexity of the environment, which is in general hard to achieve in regular training environments. Self-play is a concept that can be dated back to TD-gammon and has been explored to fit real tasks in AlphaGo, Dota2, etc. In a self-play game, the opponent agent are providing sufficient diversified responses, which resolves the issue of the lack of complexity in the environment. Moreover, the opponent agent are providing the trained agent with the right curriculum to learn.

More specifically, in this post, our main target is to implement a toy 2 vs 2 soccer game in Unity (as shown in Fig. 2) and investigate the performance of self-play. In this game, there are two teams with two agents in each. The goal is to get the ball into the opponent's goal while preventing the ball from entering own goal.

<figure align="center">
  <img width="80%" src="../../../assets/images/team05/soccer.png">
  <figcaption>Fig 2. 2 vs 2 soccer game in Unity [2].</figcaption>
</figure>


To achive this, we are going to firstly build up the soccer environment in Unity [2]. Then we plan to create a 1 vs 1 soccer game to investigate the behaviors of competition. After the 1 vs 1 scenario works, we will handle the 2 vs 2 soccer game which involves cooperation too.

## Relavant Papers and Methodology

We plan to implement some of the below **MARL algorithms** [3] and analyse their behaivors:

- Self-play
  - [Emergent complexity via multi-agent competition](https://arxiv.org/pdf/1710.03748.pdf) [15]

- Independent Learning
  - [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/1803.11485.pdf) [4]
- Value Decomposition
  - [VDN：Value-Decomposition Networks For Cooperative Multi-Agent Learning](https://arxiv.org/pdf/1706.05296) [5]
  - [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf) [6]
  - [QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1905.05408) [7]
- Policy Gradient
  - [COMA：Counterfactual Multi-Agent Policy Gradients](https://arxiv.org/abs/1705.08926) [8]
  - [MADDPG：Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf&quot;&gt;Multi-Agent) [9]
- Communication
  - [BiCNet：Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games](https://arxiv.org/abs/1703.10069) [10]
  - [CommNet：Learning Multiagent Communication with Backpropagation](https://arxiv.org/abs/1605.07736) [11]
  - [IC3Net：Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks](https://arxiv.org/abs/1812.09755) [12]
  - [RIAL/RIDL：Learning to Communicate with Deep Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1605.06676) [13]
- Exploration
  - [MAVEN：Multi-Agent Variational Exploration](https://arxiv.org/pdf/1910.07483) [14]




Hopefully, we can build a game environment like this:

<p align="center">
	<iframe width="618" height="473" src="https://www.youtube.com/embed/Hg3nmYD3DjQ" frameborder="0" allowfullscreen ng-show="showvideo"></iframe>
</p>

Specicfically, two team contains both a *striker* and a *goalie*, and each trained using separate reward functions and brains. 

| Striker Goal    | Get the ball into the oppoents goal                         |  |  |
| --------------- | ----------------------------------------------------------- |  |  |
| Goalie Goal     | Defend own goal from oppoents                               |  |  |
| Observations    | Local ray-case perception on nearby objects                 |  |  |
| Actions         | Movement and rotation in x, z plane                         |  |  |
| Striker Rewards | +1 when its team scores goal; -0.1 when oppoent scores goal |  |  | 
| Goalie Rewards  | +1 when its team scores goal; -0.1 when oppoent scores goal |  |  |

## Conclusion

We will also create a GUI to allow users to flight with RL agents, if time allows.

It is possible that some algorithms don't work well in the soccer settings. We will try to finetune the hyper-parameters and analyse why some of the algorithms fail.

By this project, we hope to provide a easy-to-use multi-agent environment and plug-and-play MARL algorithms to class and, also give detailed analysis of behaviors of MARL agents to readers.



## Other Potential Environments
- [Multi-agent tennis](https://github.com/kantologist/multiagent-sac)
- [Competitive pone](https://github.com/ucla-rlcourse/competitive-rl)
- [Competitive car-racing](https://github.com/ucla-rlcourse/competitive-rl)

## Implementation

### Environment setup

Follow the [instruction](https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/Installation.md) to setup environment. Specifically, you need to:

1. Install Unity 2021.3.11f1 from [Unity Hub](https://unity3d.com/get-unity/download).
2. Clone the [repo](https://github.com/liqi0126/unity_soccer). 
3. Install `com.unity.ml-agents` and `com.unity.ml-agents.extensions` in Unity.
4. Install `ml-agents` and `torch` in conda environment.

### MARL Training

1. Download [SoccerTwo.app](https://web.cs.ucla.edu/~liqi0126/files/SoccerTwo.app.tar) and uncompress. You can also follow the instruction from [here](https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/Learning-Environment-Executable.md) to build SoccerTwo.app by yourself.
2. In the github repo, run

``` 
mlagents-learn config/poca/SoccerTwos.yaml  --env=SoccerTwo --run-id=[job id]
```

3. You can edit `SoccerTwos.yaml` to try new algorithms
4. You can monitor the training process in tensorboard with `tensorboard --logdir results`.

### Testing

1. The SoccerTwos.onnx in your `results/[your job id]/` folder is the checkpoint for you MARL algorithm.
2. Follow the step [here](https://github.com/Unity-Technologies/ml-agents/blob/release_19_docs/docs/Getting-Started.md#embedding-the-model-into-the-unity-environment) to test your model performance in Unity environmnt.

## Trainer

### MA-POCA
In this project, we first provide [MA-POCA](https://arxiv.org/pdf/2111.05992.pdf) (MultiAgent POsthumous Credit Assignment), a neural network that acts as a "coach" for a whole group of agents. Since Muti-Agent game needs to be considered with functionality for training cooperative behaviors - i.e., groups of agents working towards a common goal, where the success of the individual is linked to the success of the whole group. In such a scenario, agents typically receive rewards as a group. You can give rewards to the team as a whole, and the agents will learn how best to contribute to achieving that reward. Agents can also be given rewards individually, and the team will work together to help the individual achieve those goals. During an episode, agents can be added or removed from the group, such as when agents spawn or die in a game. If agents are removed mid-episode (e.g., if teammates die or are removed from the game), they will still learn whether their actions contributed to the team winning later, enabling agents to take group-beneficial actions even if they result in the individual being removed from the game (i.e., self-sacrifice). MA-POCA can also be combined with self-play to train teams of agents to play against each other.


## References

1. Yang, Yaodong, and Jun Wang. “An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective.” ArXiv.org, 18 Mar. 2021, https://arxiv.org/abs/2011.00583. 
2. https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos
3. https://github.com/TimeBreaker/MARL-papers-with-code/blob/main/README.md
4. Rashid, Tabish, et al. “QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.” ArXiv.org, 6 June 2018, https://arxiv.org/abs/1803.11485. 
5. Sunehag, Peter, et al. “Value-Decomposition Networks for Cooperative Multi-Agent Learning.” ArXiv.org, 16 June 2017, https://arxiv.org/abs/1706.05296v1. 
6. Rashid, Tabish, et al. "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning." 2018, http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf. 
7. Son, Kyunghwan, et al. “QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning.” ArXiv.org, 14 May 2019, https://arxiv.org/abs/1905.05408. 
8. Foerster, Jakob, et al. “Counterfactual Multi-Agent Policy Gradients.” ArXiv.org, 14 Dec. 2017, https://arxiv.org/abs/1705.08926. 
9. Lowe, Ryan, et al. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.” ArXiv.org, 14 Mar. 2020, https://arxiv.org/abs/1706.02275. 
10. Peng, Peng, et al. “Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-Level Coordination in Learning to Play Starcraft Combat Games.” ArXiv.org, 14 Sept. 2017, https://arxiv.org/abs/1703.10069. 
11. Sukhbaatar, Sainbayar, et al. “Learning Multiagent Communication with Backpropagation.” ArXiv.org, 31 Oct. 2016, https://arxiv.org/abs/1605.07736. 
12. Singh, Amanpreet, et al. “Learning When to Communicate at Scale in Multiagent Cooperative and Competitive Tasks.” ArXiv.org, 23 Dec. 2018, https://arxiv.org/abs/1812.09755. 
13. Foerster, Jakob N., et al. “Learning to Communicate with Deep Multi-Agent Reinforcement Learning.” ArXiv.org, 24 May 2016, https://arxiv.org/abs/1605.06676. 
14. Mahajan, Anuj, et al. “Maven: Multi-Agent Variational Exploration.” ArXiv.org, 20 Jan. 2020, https://arxiv.org/abs/1910.07483. 
15. Bansal T, Pachocki J, Sidor S, et al. Emergent complexity via multi-agent competition[J]. arXiv preprint arXiv:1710.03748, 2017.
