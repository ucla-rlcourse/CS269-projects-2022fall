---
layout: post
comments: true
title: 2 vs 2 soccer game
author: Linqiao Jiang, Qi Li, Huizhuo Yuan (Team 05)
date: 2022-09-19
---


> In this project we are going to investiage **self-play for compettive Multi-agent Reinforcement Learning (MARL)**. We will build a 2 vs 2 soccer game in Unity and try different MARL algorithms on it. We will provide the source code and detailed analysis.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

---

## Project Proposal 

In this project we will investigate **self-play for compettive Multi-agent Reinforcement Learning (MARL)**. Different from single-agent systems, the evolution of the environmental state and the reward function that each agent received are not only determined by the envrionment, but also other agents' joint actions. As a result, agents need to take into account and interact with not only the environment but also other intelligent agents (see Fig. 1). In a multi-agents scenario, there could be cooperation (e.g., May and Cody in video game *It Takes Two*) and competition (e.g., most of board games).

![MARL]({{ '/assets/images/team05/MARL.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. Single-agent vs Multi-agent* [1].

Concretely, our target is to implement a toy 2 vs 2 soccer game in Unity (as shown in Fig. 2). In this game, there are two teams with two agents in each. The goal is to get the ball into the opponent's goal while preventing the ball from entering own goal.

![soccer]({{ '/assets/images/team05/soccer.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 2. 2 vs 2 soccer game in Unity* [2].

To achive this, we are going to firstly build up the soccer environment in Unity [2]. Then we plan to create a 1 vs 1 soccer game to investigate the behaviors of competition. After the 1 vs 1 scenario works, we will handle the 2 vs 2 soccer game which involves cooperation too.

We plan to try different **MARL algorithms** [3] and analyse their behaivors:

- Independent Learning
  - [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/1803.11485.pdf) [4]

- Value Decomposition
  - [VDN：Value-Decomposition Networks For Cooperative Multi-Agent Learning](https://arxiv.org/pdf/1706.05296) [5]
  - [QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning](http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf) [6]
  - [QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1905.05408) [7]

- Policy Gradient
  - [COMA：Counterfactual Multi-Agent Policy Gradients](https://arxiv.org/abs/1705.08926) [8]
  - [MADDPG：Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf&quot;&gt;Multi-Agent) [9]

- Communication
  - [BiCNet：Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games](https://arxiv.org/abs/1703.10069) [10]
  - [CommNet：Learning Multiagent Communication with Backpropagation](https://arxiv.org/abs/1605.07736) [11]
  - [IC3Net：Learning when to Communicate at Scale in Multiagent Cooperative and Competitive Tasks](https://arxiv.org/abs/1812.09755) [12]
  - [RIAL/RIDL：Learning to Communicate with Deep Multi-Agent Reinforcement Learning](https://arxiv.org/abs/1605.06676) [13]

- Exploration
  - [MAVEN：Multi-Agent Variational Exploration](https://arxiv.org/pdf/1910.07483) [14]

We will also create a GUI to allow users to flight with RL agents, if time allows.

It is possible that some algorithms don't work well in the soccer settings. We will try to finetune the hyper-parameters and analyse why some of the algorithms fail.

By this project, we hope to provide a easy-to-use multi-agent environment and plug-and-play MARL algorithms to class and also, give detailed analysis of behaviors of MARL agents to readers.

## References

1. Yang, Yaodong, and Jun Wang. “An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective.” ArXiv.org, 18 Mar. 2021, https://arxiv.org/abs/2011.00583. 
2. https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#soccer-twos
3. https://github.com/TimeBreaker/MARL-papers-with-code/blob/main/README.md
4. Rashid, Tabish, et al. “QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.” ArXiv.org, 6 June 2018, https://arxiv.org/abs/1803.11485. 
5. Sunehag, Peter, et al. “Value-Decomposition Networks for Cooperative Multi-Agent Learning.” ArXiv.org, 16 June 2017, https://arxiv.org/abs/1706.05296v1. 
6. Rashid, Tabish, et al. "QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning." 2018, http://proceedings.mlr.press/v80/rashid18a/rashid18a.pdf. 
7. Son, Kyunghwan, et al. “QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning.” ArXiv.org, 14 May 2019, https://arxiv.org/abs/1905.05408. 
8. Foerster, Jakob, et al. “Counterfactual Multi-Agent Policy Gradients.” ArXiv.org, 14 Dec. 2017, https://arxiv.org/abs/1705.08926. 
9. Lowe, Ryan, et al. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.” ArXiv.org, 14 Mar. 2020, https://arxiv.org/abs/1706.02275. 
10. Peng, Peng, et al. “Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-Level Coordination in Learning to Play Starcraft Combat Games.” ArXiv.org, 14 Sept. 2017, https://arxiv.org/abs/1703.10069. 
11. Sukhbaatar, Sainbayar, et al. “Learning Multiagent Communication with Backpropagation.” ArXiv.org, 31 Oct. 2016, https://arxiv.org/abs/1605.07736. 
12. Singh, Amanpreet, et al. “Learning When to Communicate at Scale in Multiagent Cooperative and Competitive Tasks.” ArXiv.org, 23 Dec. 2018, https://arxiv.org/abs/1812.09755. 
13. Foerster, Jakob N., et al. “Learning to Communicate with Deep Multi-Agent Reinforcement Learning.” ArXiv.org, 24 May 2016, https://arxiv.org/abs/1605.06676. 
14. Mahajan, Anuj, et al. “Maven: Multi-Agent Variational Exploration.” ArXiv.org, 20 Jan. 2020, https://arxiv.org/abs/1910.07483. 
