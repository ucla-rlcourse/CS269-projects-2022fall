---
layout: post
comments: true
title: Reinforcement Learning for Football Player Training
author: Justin Cui (Team 06)
date: 2022-11-13
---


> As the most popular sport on the planet, millions of fans enjoy watching Sergio Ag√ºero, Raheem Sterling, and Kevin de Bruyne on the field. Football video games are less lively, but still immensely popular, and we wonder if AI agents would be able to play those properly. Researchers want to explore AI agents' ability to play in complex settings like football. The sport requires a balance of short-term control, learned concepts such as passing, and high-level strategy, which can be difficult to teach agents. We apply RL to train the agents to see if we can achieve good performances.
<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
In this work, we will utilize the football simulation platform provided by Google Research. The goal is to train an intelligent agent to play football that can achieve the highest score. More detailed information can be found in [Google Research Football with Manchester City F.C.
](https://www.kaggle.com/competitions/google-football)

The whole project can be divided into multiple states. First of all, we will develop simple rule-based agents to collect training episodes. After that, we will use these collected episodes to train our deep learinng model based agents. In the end we will show the superior performances of our model based agents against the baseline agents. After that we will creatively combine the two and show how to influence the agents to favor certain types of tactics which are critical for football coaches.

![YOLO]({{ '/assets/images/team06/screenshot_01.png'|relative_url}}){: style="width: 600px; max-width: 100%;"}
The following is a demo of the training process
![Simulation]({{ '/assets/images/team06/simulation.gif'|relative_url}}){: style="width: 600px; max-width: 100%;"}

## Environment
We mainly used 2 environments in the project. The first environment is the Google SMM football environment integrated with Gym. This enables us to quickly iterate on our algorithms. The second envoronment is the Google football simulator which renders realistic football games. We mainly use the first environment for development and the latter for demonstration.
![SMM Environment]({{ '/assets/images/team06/smm_env_replay.gif'|relative_url}}){: style="width: 600px; max-width: 100%;"}

## Algorithm

### Input
As we are processing sequences of images(video) as input, there are multiple ways that we can design the input. One way is to feed the last N images to our model, this works very effectively for Pong, however, it's shown to be less effective in our case. The other way is to compute the difference between current image and the past images and feed the difference to the algorithm. This turns out to be much more effective than using the past few images. In our algorithm, we will follow th latter.

### Interaction with the environment
We created a wrapper class to encapsulate the interactions with the environment such as caching the reply, computing the difference, etc.
```
class SMMFrameProcessWrapper(gym.Wrapper):
    def build_buffered_obs(self) -> np.ndarray:
        """
        Iterate over the last dimenion, and take the difference between this obs 
        and the last obs for each.
        """
        agg_buff = np.empty(self._obs_shape)
        for f in range(self._obs_shape[-1]):
            agg_buff[..., f] = self._obs_buffer[1][..., f] - self._obs_buffer[0][..., f]

        return agg_buff

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict[Any, Any]]:
        """Step env, add new obs to buffer, return buffer."""
        obs, reward, done, info = self.env.step(action)

        obs = self._normalise_frame(obs)
        self._obs_buffer.append(obs)

        return self.build_buffered_obs(), reward, done, info

    def reset(self) -> np.ndarray:
        """Add initial obs to end of pre-allocated buffer.

        :return: Buffered observation
        """
        self._prepare_obs_buffer()
        obs = self.env.reset()
        self._obs_buffer.append(obs)

        return self.build_buffered_obs()
```
### Observation processing
We use a convolutional network to process the inputs and convert them into observations. Below is an illustration of our model.
![Observation]({{ '/assets/images/team06/model.png'|relative_url}}){: style="width: 600px; max-width: 100%;"}

### Agent
We use a DQN network as the agent. The architecture of DQN is described as below
![DQN]({{ '/assets/images/team06/dqn.jpeg'|relative_url}}){: style="width: 600px; max-width: 100%;"}
We are currently using the implementaion from [reinforcement-learning-keras](https://github.com/garethjns/reinforcement-learning-keras) as the baseline, we will add more details in the next update.




<!-- ## Main Content
Your article starts here. You can refer to the [source code](https://github.com/lilianweng/lil-log/tree/master/_posts) of [lil's blogs](https://lilianweng.github.io/lil-log/) for article structure ideas or Markdown syntax. We've provided a [sample post](https://ucla-rlcourse.github.io/CS269-projects-2022fall/2017/06/21/an-overview-of-deep-learning.html) from Lilian Weng and you can find the source code [here](https://github.com/ucla-rlcourse/CS269-projects-2022fall/blob/main/_posts/2017-06-21-an-overview-of-deep-learning.md)

## Basic Syntax
### Image
Please create a folder with the name of your team id under `/assets/images/`, put all your images into the folder and reference the images in your main content.

You can add an image to your survey like this:
![YOLO]({{ '/assets/images/team00/object_detection.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. YOLO: An object detection method in computer vision* [1].

Please cite the image if it is taken from other people's work.


### Table
Here is an example for creating tables, including alignment syntax.

|             | column 1    |  column 2     |
| :---        |    :----:   |          ---: |
| row1        | Text        | Text          |
| row2        | Text        | Text          |



### Code Block
```
# This is a sample code block
import torch
print (torch.__version__)
```


### Formula
Please use latex to generate formulas, such as:

$$
\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}
$$

or you can write in-text formula $$y = wx + b$$. -->

<!-- ### More Markdown Syntax
You can find more Markdown syntax at [this page](https://www.markdownguide.org/basic-syntax/). -->

## Reference

[1] Kurach, K., Raichuk, A, et al. "Google Research Football: A Novel Reinforcement Learning Environment" Proceedings of the AAAI Conference on Artificial Intelligence. 2020.


