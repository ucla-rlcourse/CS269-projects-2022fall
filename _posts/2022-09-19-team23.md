---
layout: post
comments: true
title: Planning with Learned Actionable Object-centric World Model
author: Ziheng Zhou, Yingqi Gao, Haoyi Qiu (team 23)
date: 2022-11-13
---

> To achieve better planning in zero-shot generalization tasks, we propose to learn imagined actions over perceived objects in the learned world model. 
> We will implement our method based on Rishi's OP3 method ([5]), which learns a general object-centric world model structure. We will create a billiard ball environment to test and improve the algorithm. We also plan to borrow some of Meta World's robotic arm control tasks ([6]) and make some creative tool usage tasks that require robots to use objects as tools without supervision and demonstration.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Project Proposal

Model-based RL can improve data efficiency and reduce the amount of data needed when switching to a new environment, but for the zero-shot scenario, it doesn't necessarily generalize to a new world different without extra techniques ([2], [3]). Recently, some model-based works take the object-centric approach to learning world models in multi-object settings and achieve much better zero-shot generalization performance since such world models can much more easily adapt to different numbers of objects ([1], [4], [5]). However, the objects in the current object-centric world model are treated implicitly and can't be utilized explicitly for planning. People still have to treat the entire object-centric world model as one black box, just like other kinds of world models during planning.

To achieve better planning in zero-shot generalization tasks, we propose to learn imagined actions over perceived objects in the learned world model. The imagined action is an interface to imagine how the object could be changed regardless of how it's achieved (the agent would change it, and the other objects can also affect it). Then, we can imagine first what 'action' should the goal object take to achieve the goal state, and then imagine what other object should 'act' to provide the goal object's desired action, and iteratively all the way back to the agent's own action space. In this way, especially in the multi-object control task, the exponentially complex planning task is decomposed into sub-problems that each is easy to plan.

We will implement our method based on Rishi's OP3 method ([5]), which learns a general object-centric world model structure. We will create a billiard ball environment to test and improve the algorithm. We also plan to borrow some of Meta World's robotic arm control tasks ([6]) and make some creative tool usage tasks that require robots to use objects as tools without supervision and demonstration.

_Note: we intend this project to be a fully academic project and publish later. We'd appreciate our fellow classmates to keep the content confidential._

## Algorithm

### Reconstruction of OP3 Algorithm

Firstly we reconstructed the OP3 algorithm that's closest to our project. Their algorithm has the basic structure to abstract entities unsupervisedly, including the recognition module, dynamics module, and observation module (Figure 1). Given a frame sequence of the dynamics of objects in pure images and the action labels, this algorithm is able to learn a world model that has an object encoding corresponding to each of the objects in the scene. The recognition module is responsible to extract such encodings from a static frame, the dynamics module is responsible for computing the next states of those objects, and the refinement module is responsible for reconstructing images from the object encodings.  

![OP3Main]({{ '/assets/images/team23/op3main.png'|relative_url}})
{: style="width: 400px; max-width: 100%;"}
*Figure 1. OP3 Main*

Specifically, the recognition module and observation module work closely by iteratively improving each other during both training and inference stage. The method based on IODINE [7] algorithm which uses armotized inference procedure to iteratively refine (see Figure 2). Inside IODINE, the reconstruction(observation module) method is seen as Figure 3. It reconstructs each object individually and add at the pixel level with a mask to decide which object to appear in case of occlusion. 

![IODINE]({{ '/assets/images/team23/IODINE.png'|relative_url}})
{: style="width: 400px; max-width: 100%;"}
*Figure 2. IODINE*

![OP3Observation]({{ '/assets/images/team23/op3observation.png'|relative_url}})
{: style="width: 400px; max-width: 100%;"}
*Figure 3. OP3 Observation*

The formula for updating the next iteration in IODINE is:
$$
z_k^t \sim q_H(z_k^t | x)
$$
$$
H_k^{t+1} \leftarrow H_k^t + f_\theta (z_k^t, x, a_k)
$$
where $$z_k^t$$ is the sampled encoding of object $$k$$ from the distribution $$H$$ given $$x$$ at $$t$$ step, and the $$H_k$$ updates itself through an neural network $$\theta$$ that takes in its previous step's sample $$z_k$$, the image $$x$$, and other inputs $$a_k$$ which includes gradients, masks, loss etc.

The loss function to optimize for IODINE is to minize the evident lower bound (ELBO) like the standard VAEs.
$$
L_t = D_{KL}(q_H(z^t|x) || p(z)) - log p(x|z^t)
$$
where $$p(z)$$ is a normal distribution in this setting.

The dynamic module holds the object encoding slots as they are and update the next states of them locally using only actions and these encodings themselves without using pixel information (see Figure 4). It firstly updates the objects state by accounting the effect of actions. With this updated encoding, then it considers the objects' interactions by calculating pairwise effects and update again.

![OP3Dynamics]({{ '/assets/images/team23/op3dynamics.png'|relative_url}})
{: style="width: 400px; max-width: 100%;"}
*Figure 4. OP3 Dynamics*

The algorithm procedure is as follows:
<ol>
    <li> Entities and action encoding:
          \begin{align*}
            \tilde{H_k} &= d_o(H_k),\\ 
            \tilde{A} &= d_a(A)
          \end{align*}
         </li>
    <li> Action intervention:  
          Measures how and to what degree an action \(A\) affects the entity \(H_k\).
          \begin{align*}
            \tilde{H_k}^{act} &= d_{ao}(\tilde{H_k}, \tilde{A})\\
            &:= d_{act-eff}(\tilde{H_k}, \tilde{A}) \cdot  d_{act-att}(\tilde{H_k}, \tilde{A})
          \end{align*}
          </li>
    <li> Pairwise interaction:  
          Measures how and to what degree other entities affect the entity \(H_k\).
            \begin{align*}
                \tilde{H_k}^{interact} &= \sum_{i\neq k}^Kd_{oo}(\tilde{H_i}^{act}, \tilde{H_k}^{act})\\
                &:= \sum_{i\neq k}^K[d_{obj-eff}(\tilde{H_i}^{act}, \tilde{H_k}^{act}) \cdot  d_{obj-att}(\tilde{H_i}^{act}, \tilde{H_k}^{act})]
            \end{align*}
            </li>
    <li> Pairwise interaction VAE:  
          Instead of learning the specific pairwise interaction, the model learns a probabilistic version of it, i.e., the parameters of its distribution, 
          \begin{align*}
              f(\tilde{H_k}^{interact}).
          \end{align*}
          Then we sample from this distribution
          $$
          \tilde{H_k}^{interact(sampled)}\sim f(\tilde{H_k}^{interact}).
          $$ </li>
    <li> Effect aggregation:  
          \begin{align*}
              H_k'=d_{comb}(\tilde{H_k}^{act}, \tilde{H_k}^{interact(sampled)})
          \end{align*}
          </li>
    <li> Posterior sampling of the next state:  
          $$
          H_k'^{(sampled)}\sim f(H_k')
          $$ </li>
</ol>

After it's trained, OP3 can be used to plan actions for tasks without any training if provided with goal state image. The method is that it tries a bunch of random actions and use the model to simulate the final results. And it selects the best several actions to then sample another batch of random actions using CEM method, but theoretically better ones now. The way it measures the "best" is through comparing the latent object enconding with the infered goal state object encoding. 


After several iterations, it selects the best action at the last trial. As we can see, this method did not use the object representations it learned explicitly. The whole model is still a blackbox. 

We've reproduced the result and started adapting it toward our own idea (see figure 5). Currently we just keep using the original paper's environment/tasks (pick and place block objects) so that we can make sure we reproduce it. We have created our own environment but we haven't put them together yet. 

![VAEInteractionNormal]({{ '/assets/images/team23/vaeInteractionNormal.jpg'|relative_url}})
{: style="width: 400px; max-width: 100%;"}
*Figure 5. VAE Interaction Normal*

### Our Method Design and Some Preliminary Results

To be able to imagine the actions or interventions over any specific object, we need to first have a method to specify the actions. Variational encodings with forced normal distribution prior is a good way to achieve this purpose. Suppose the action encodings are trained well and close enough to normal distribution, we would have a known prior that we can sample from to generate various actions. 

Second, we need to make sure the actions we specify can affect the objects in ways it "normally" would change. That means, it should change like the kinds of changes it experienced during training process. And for any kind of changes it experienced, we should be able to reproduce it. So our variational encoding should cover all kinds of actions it experienced by not more. The current dynamic module workflow of OP3 would not fit our purpose as it separates agents' actions from the objects interactions. This mechanism would make it much harder since the interaction effects are harder to control and to make sure the training dataset to be diverse enough to cover all kinds of changes it could experience. Moreover, it prevents us from learning one unified action interface which is more physically realistic (they are both forces, after all) and also more elegant. Therefore we changed the dynamic part to be as follows:
<ol>
    <li> We skipped the entities encoding parts that accounts for the inertia in the original algorithm because we want it to be implicitly learned in one same network that accounts for the actions effect. That means, when the action is absent, that network should output the effect from the inertia.</li> 
    <li> Action intervention:  
          In our design, the action come from an agent that's either also in the scene or can only affect one object at a time. So we want to ground the actions on to the object it affect (the object could be agent itself). So we use an attention to select the object to affect on. We also encode the action to be a vector encoding like the old algorithm, but we don't hurry to apply the action to objects and get an updated object encoding for now.
          \begin{align*}
            A^{grounded} = d_{act-enc}(A) \cdot  d_{act-att}(H_{1:K}, A)
          \end{align*}
         </li>
    <li> Pairwise interaction:  
          We still want to measure how and to what degree other entities affect the entity \(H_k\). This interaction is not from agents actions of course. 
            \begin{align*}
                H_k^{pair} = \sum_{i\neq k}^K[d_{obj-imp}(H_i, H_k) \cdot  d_{obj-att}(H_i, H_k)]
            \end{align*}
         </li>
    <li> Impact aggregation:  
          This is also where we are different. We treat impact from action and from objects' interaction to be intrinsically same type (intuitively, they are all just force). For terminology wise, we use "impact" to denote this general force from both action and interaction. In this step, we aggregate both kinds of impacts toward a specific entity \(H_k\) by simply adding them up:
          $$
          H_k^{impact} = A^{grounded} + H_k^{pair}
          $$ </li>
    <li> Total impact VAE:  
          In this step we turn the aggregated impact into a variational encoding. This is how we manage to have one unified "action" interface toward any object. No matter where the original impact come from (from agents' actions or objects interactions), the aggregated impact at any time step must follow a prior distribution. Thus we create an interface to intervene the object agnostic to the impact source, and thus we can imagine intervening the objects later.
          $$
          H_k^{impact(sampled)}\sim f(H_k^{impact}).
          $$ </li>
    <li> Update the objects by applying the impact:  
          At this step we apply the aggregated impacts toward the object to update its state. We use a network with the object's previous state \(H_k\) and the total impact sampled from the learned distribution \(H_k^{impact(sampled)}\) as the inputs and compute the state change. And then we add the state change to it to get the object's next state. 
          \begin{align*}
              \Delta H_k&=d_{effect}(H_k^{impact(sampled)}, H_k)\\
              H_k'&=H_k+\Delta H_k
          \end{align*}
          </li>
    <li> Posterior sampling of the next state:
          This part remains the same.
          $$
          H_k'^{(sampled)}\sim f(H_k')
          $$ </li>
</ol>

We have almost finished the algorithm implementation, but have not being able to finish training yet. 

Even though we critisize the original methods separation of action and interactions, at the beginning we still want to just reproduce first and change slowly. And as an experimental move, we wanna try to investigate the variational encoding and see its representation power. So before we finish all the algorithm, we first conducted an experiment by only changing the objects' pairwise interaction to be variational encoding and check the reconstruction result. 

As we can see in figure 5, the reconstruction result is almost perfect when we use the interaction effect encoding normally as what the network compute is. Then when we apply random interaction by randomly sampling from normal distribution (Figure 6), the result maintains perfect at the first step, and starts to slowly degrade later. We are relatively satisfied with this results since it shows that it basically works, but also it shows that we need more mechanism to ensure the ability to apply more steps. Maybe (most likely) it's because the interaction distribution is not really normal distribution so our sample may go out of distribution. We may want to use a learned distribution to sample in the future. 

![VAEInteractionSample]({{ 'assets/images/team23/vaeInteractionSample.jpg'|relative_url}})
{: style="width: 400px; max-width: 100%;"}
*Figure 6. VAE Interaction Sample*

## Evaluation
### Billiard Ball Task

We simplify and define the billiard ball game environment and rules to:
<ol>
    <li> There are one white ball and one to \(\infty\) number(s) of solid color ball(s). </li>
    <li> There are at most six pocket(s) on the billiard ball table. </li>
    <li> Our goal is to hit all solid color ball(s) into the pocket(s) with the help of the white ball or the consequence of the collision(s) of the white ball and the other solid color ball(s). </li>
    <li> There are no limited numbers of white ball hit action in one episode. </li>
    <li> If the white ball goes into the pocket, the game will not end; instead, we will place the white ball again on the billiard ball table. However, there is a negative reward (punishment) in this situation. </li>
    <li> There are positive rewards if the solid color ball(s) go into the pocket(s) after hitting a white ball. If there is no solid color ball(s) going into the pocket after hitting a white ball, the reward will be calculated based on the distance between the solid color ball(s) and the nearest pocket(s). </li>
</ol>

We implement our environment based on two existing **(BiB)** billiard ball game environment: **Bouncing Ball (BB)** [9] and **Pool Game (PG)** [10]. After modifications, we can apply the above proposed novel algorithm and some baseline algorithms to this environment to their performance.

![PoolGameEnvironment]({{ 'assets/images/team23/pg-1.png'|relative_url}})
{: style="width: 400px; max-width: 100%;"}
*Figure 7. Pool Game Environment*

### Baselines

We select the model-free RL learning algorithm - PPO (Proximal Policy Optimization) [8] as one of our baseline. The algorithm is implemented with the help of **Stable Baselines3 (SB3)** [11]. We train the agent with default settings and obtain the following reward figure:

![PPOReward]({{ 'assets/images/team23/ppo-rewards.png'|relative_url}})
{: style="width: 400px; max-width: 100%;"}
*Figure 8. PPO Rewards*

...task descriptions here..

...baseline descriptions and performance with figures///


## Next Steps
First, we need to train our own algorithm on the original and the new billiard ball environment that we just created. We need to find some mechanism to ensure our action samples to be stable after enough long steps. It may need us to fit a distribution of the learned action encodings. Besides, the reward does not increase consistently as we expect, we will go over the reward function design again to make sure the training process performs as normal.
Second, we still need to finish planning algorithm during zero shot testing. 

<!-- ## Main Content
Your article starts here. You can refer to the [source code](https://github.com/lilianweng/lil-log/tree/master/_posts) of [lil's blogs](https://lilianweng.github.io/lil-log/) for article structure ideas or Markdown syntax. We've provided a [sample post](https://ucla-rlcourse.github.io/CS269-projects-2022fall/2017/06/21/an-overview-of-deep-learning.html) from Lilian Weng and you can find the source code [here](https://github.com/ucla-rlcourse/CS269-projects-2022fall/blob/main/_posts/2017-06-21-an-overview-of-deep-learning.md)

## Basic Syntax
### Image
Please create a folder with the name of your team id under `/assets/images/`, put all your images into the folder and reference the images in your main content.

You can add an image to your survey like this:
![YOLO]({{ '/assets/images/team00/object_detection.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. YOLO: An object detection method in computer vision* [1].

Please cite the image if it is taken from other people's work.


### Table
Here is an example for creating tables, including alignment syntax.

|             | column 1    |  column 2     |
| :---        |    :----:   |          ---: |
| row1        | Text        | Text          |
| row2        | Text        | Text          |



### Code Block
```
# This is a sample code block
import torch
print (torch.__version__)
```


### Formula
Please use latex to generate formulas, such as:

$$
\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}
$$

or you can write in-text formula $$y = wx + b$$.

### More Markdown Syntax
You can find more Markdown syntax at [this page](https://www.markdownguide.org/basic-syntax/). -->

## Reference

[1] Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models, 2019.

[2] Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt ̈aschel. A survey of generalisation in deep reinforcement learning. CoRR, abs/2111.09794, 2021.

[3] Thomas M. Moerland, Joost Broekens, Aske Plaat, and Catholijn M. Jonker. Model-based reinforcement learning: A survey, 2020.

[4] Haozhi Qi, Xiaolong Wang, Deepak Pathak, Yi Ma, and Jitendra Malik. Learning long-term visual dynamics with region proposal interaction networks. CoRR, abs/2008.02265, 2020.

[5] Rishi Veerapaneni, John D. Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu, Joshua B. Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement learning. CoRR, abs/1910.12827, 2019.

[6] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. CoRR, abs/1910.10897, 2019.

[7] Klaus Greff, Rapha ̈el Lopez Kaufman, Rishabh Kabra, Nick Watters, Chris Burgess, Daniel Zoran, Loic Matthey, Matthew M. Botvinick, and Alexan-der Lerchner. Multi-object representation learning with iterative variational inference. CoRR, abs/1903.00450, 2019.

[8] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017.

[9] https://github.com/jcoreyes/OP3/blob/master/op3/envs/bouncing_balls/bouncing_balls_generation.py

[10] https://github.com/packetsss/youtube-projects/tree/main/pool-game

[11] https://github.com/DLR-RM/stable-baselines3


---


<!-- ## Data Rich and Physics Certain

| Experiment 					| Parameters  											| Results  								| Comments 							|
| :---       					|    :----:   											|     :---: 							|     ---: 							|
| **DL + Data**																																						|

| Predicting only velocity  	| Dataset size : 10000<br> Network : 2->5->5->1 <br> activation: ReLU	|  ~100% accurate	| Generalises well over various initial velocities |
| Predicting only displacement 	| Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable		| Better prediction for $u_0 \in dataset$, average prediction outside | 
| Predicting both $v_t, s_t$	| Dataset size : 10000<br> Network : 2->16->16->2 <br>	activation: tanh	|	Reasonable		| Better prediction for $u_0 \in dataset$, poor prediction outside |

-----

| **DL + Physics**																																			|
| Predicting both $v_t, s_t$, using Loss $L_{physics} = \|v_{predicted}^2-u_{initial}^2-2*g*s_{predicted}\|$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	~0% accuracy		| Expected result as no supervision of any kind is provided |
| Predicting both $v_t, s_t$, using Loss $L_{velocity+phy} = (v_{predicted}-v_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Prediction of $v_t$ is good. Was able to learn $s_t$ reasonably well without direct supervision |
| Predicting both $v_t, s_t$, using Loss $L_{supervised+phy} = (v_{predicted}-v_{actual})^2+(s_{predicted}-s_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Not a better result w.r.t direct supervision |


**Observations :** 
- Physics equations are certain in this case and are the best to use.
- Both DL, Hybrid(DL+Physics) methods performance are equivalent (actual accuracy/loss varies based on fine training, random dataset generation)

Re running the above experiments with Dataset size of 200(Data Starvation), yielded the following observations
- DL performance is comparable with 10000 dataset when trained on much mode epochs(5x)
- Hybrid(DL+Physics) without direct supervision on $s_t$ has comparable/better closeness than DL only method for limited epochs($\sim$300) training.




## Data Rich and Physics Uncertain

| Experiment 					| Parameters  											| Results  								| Comments 							|
| :---       					|    :----:   											|     :---: 							|     ---: 							|
| **DL + Data**																																						|\
| Predicting both $v_t, s_t$	| Dataset size : 10000<br> Network : 2->16->16->2 <br>	activation: tanh	|	Reasonable		| Better prediction for $u_0 \in dataset$, poor prediction outside |
| **DL + Physics**																																			|
| Predicting both $v_t, s_t$<br> using Loss $L_{physics} = \|v_{predicted}^2-u_{initial}^2-2*g*s_{predicted}\|$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	~0% accuracy		| Expected result as no supervision of any kind is provided |
| Predicting both $v_t, s_t$<br> using Loss $L_{velocity+phy} = (v_{predicted}-v_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Prediction of $v_t$ is good. Was able to learn $s_t$ reasonably well without direct supervision |
| Predicting both $v_t, s_t$<br> using Loss $L_{supervised+phy} = (v_{predicted}-v_{actual})^2+(s_{predicted}-s_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Not a better result w.r.t direct supervision, but bettr than DL when $u0$ is out of dataset |


**Observations :** 
- Both DL, Hybrid(DL+Physics) methods performance are similar, Hybrid(DL+Physics) is better when $u0$ is out of dataset, DL is better for $u0$ in dataset.
- Physics equations are not certain in this case and the above methods are better to use than Physics.

## Data Starvation and Physics Uncertain
- Similar observations as in data rich -->


