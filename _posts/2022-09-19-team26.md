---
layout: post
comments: true
title: Policy Gradient in Active Learning of Graph Neural Networks
author: Yifu Yuan, Dadian Zhu
date: 2022-09-19
---


> Neural Networks have been proven to be powerful and effective in many tasks. However, training such networks usually requires huge amount of data. In supervised learning paradigm, labelling data involves professional domain knowledge and thus becomes expensive and sometimes impratical. To address the data shortage, active learning mothods have been proposed which focuses on how to efficiently label most valuable data samples to reduce annotation cost. In this project, we investigate the application of Policy Gradient in active learning of Graph Neural Networks (GNN). We mainly focus on the affect of better states representation and effectiveness of different policy gradient backbones.


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## 1. Introduction
Graph is a natural way to represent the connections of interacting objects, such as networking, trafficing and so on. Thus, GNN as an effective way to learn the representations of graphs has been very successful in various tasks. In both node prediction and link prediction tasks, GNN has shown the ability to learn the intertwining relationships between entities. However, GNN usually requires large dataset to learn in the first place. In supervised learning tasks, labelled data with high quality is expensive, a problem that has attracted focus.

Active learning is brought to tackle this issue. It dynamically selects the most valuable or informative samples for an oracle to label and iteratively trains the target model. An advantage of active learning in GNN is that we can mostly exploit the high correlation among nodes to select and label the most informative sample.

The process of selecting samples to label, putting it back for training and repeating resembles Markov Decision Process, as the states being selected samples and actions being choosing next one to label. Thus, it is natural to apply reinforcement learning in such task to find optimal strategy in selecting the most apporiate samples.


## 2. Related Work
Active learning in GNN has been an active research field [1] [2] [3]. In these works, researchers mainly focus on using different metrics and better feature propogation methods to select the most informative nodes. 

Some other works exert reinforment learning in GNN active learning [4] [5] [6]. 

## 3. Our Work
Following the setup of [6], we investigate the influence of states representation and effectiveness of different reinforment learning backbones, as well as a contrast between abundant rewards with high variance and sparse rewards with low variance.

### Setup and Problem Definition
(Please check [6] for now; we will provide this in the final report)

### Our Adjustments
#### State Representation
One natural problem is whether a more powerful and informative state representation will better guide the policy network. To accomplish this, we propose two novel metrics along with a well known PageRank centrality score to facilitate the state representation.

**Feature Dissimilarity Score (FDS)**:   To evaluate how dissimilar a candidate node is compared to the nodes that have already been labeled in terms of their node feature, we propose FDS, which applies the commonly used cosine similarity for feature distance measurement. Let $$FEA(v_i)$$ be the feature vector of node $$v_i$$, then: $$\phi_{FDS}(v_i)=\frac{1}{max_{l\in L_t}cos(FEA(v_i),FEA(l))}$$

**Structural dissimilarity score (SDS)**: Given the adjacency matrix $$A$$, the element $$(A^2)_{i, j}$$ gives the number of paths of length 2 from node $$i$$ to node $$j$$, i.e. number of shared neighbors between node $$i$$ and node $$j$$. Therefore, we could design our score $$\phi_{SDS}$$ to be: $$\phi_{SDS} (v_{i}) = \frac{1}{max_{l \in L_t} (A^2)_{i,index(l)}}$$, where $${index}(l)$$ is the index of the labeled node $$l$$.

With the three newly added heuristic criteria, intuitively we have a more powerful state representation. We investigate the influence of such representation.

#### Sparse Rewards v.s. Abundunt Rewards
[6] uses the validation accuracy after GNN convergence as the rewards. Such rewards are sparse as they are obtained once in each epoch. We choose to use the validation accuracy after each sample selection as rewards to train the policy network. As these rewards have high variance, we need to subtract a baseline, which is the moving average of previous validation accuracies.

#### Policy Gradient Backbones
[6] uses REINFORCE solely. We change the backbone to be PPO [7] (already implemented) and actor critic (trying to implement) to see whether it affects performance.

**(No results yet, still tuning as it takes too much time)**

## References
[1] Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. Active learning for graph embedding. [https://arxiv.org/pdf/1705.05085.pdf](https://arxiv.org/pdf/1705.05085.pdf)

[2] Lan-Zhe Guo, Tao Han, and Yu-Feng Li. Robust Semi-Supervised Representation Learning for Graph-Structured Data. [http://www.lamda.nju.edu.cn/guolz/pakdd.pdf](http://www.lamda.nju.edu.cn/guolz/pakdd.pdf)

[3] Yuexin Wu, Yichong Xu, Aarti Singh, Yiming Yang, and Artur Dubrawski. Active learning for graph neural networks via node feature propagation. [https://grlearning.github.io/papers/46.pdf](https://grlearning.github.io/papers/46.pdf)

[4] Xiaowei Jia, Beiyu Lin, Jacob Zwart, Jeffrey Sadler, Alison Appling, Samantha Oliver, Jordan Read. Graph-based Reinforcement Learning for Active Learning in Real Time: An Application in Modeling River Networks. [https://arxiv.org/pdf/2010.14000.pdf](https://arxiv.org/pdf/2010.14000.pdf)

[5] Yuheng Zhang, Hanghang Tong, Yinglong Xia, Yan Zhu, Yuejie Chi, Lei Ying, Batch Active Learning with Graph Neural Networks via Multi-Agent Deep Reinforcement Learning. [https://ojs.aaai.org/index.php/AAAI/article/view/20897](https://ojs.aaai.org/index.php/AAAI/article/view/20897)

[6] Shengding Hu, Zheng Xiong, Meng Qu, Xingdi Yuan, Marc-Alexandre Côté, Zhiyuan Liu, and Jian Tang. Graph Policy Network for Transferable Active Learning on Graphs. [https://arxiv.org/pdf/2006.13463.pdf](https://arxiv.org/pdf/2006.13463.pdf)

[7] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. Proximal Policy Optimization Algorithms. [https://arxiv.org/pdf/1707.06347.pdf](https://arxiv.org/pdf/1707.06347.pdf)






