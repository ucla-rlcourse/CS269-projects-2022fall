---
layout: post
comments: true
title: "\"Explorations\" on Go-Explore: a New Approach for Hard-Exploration Problems"
author: Yifu Yuan, Dadian Zhu (Team 26)
date: 2022-09-19
---


> Intelligent exploration of a reinforcement learning model has been an active area of research. In such problem, hard-exploration is of particular interest. Under this paradigm, most existing RL algorithms perform poorly as the rewards are sparse and even misleading. To address the shortfall, Go-Explore (Ecoffet et al., 2021) contibutes a distinct strategy by (1) remembering previously visited states, (2) first returning to a promising state, then exploring from it, and (3) solving simulated environments, then robustifying via imitation learning. The above principles combined dramatically improve the performance on hard-exploration problems. Yet, some modules of Go-Explore (Ecoffet et al., 2021) remain primitive and leave room for more comprehensive experiments and even optimizations. Thus, in our work, we focus on the following aspects: (1) cell (game states) representation, (2) cell heuristics, (3) stochastical over deterministic training, (4) cell exploration strategy and (5) learning from demonstration algorithm

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction to Go-Explore
### Go-Explore Algorithm
The key insight of this algorithm is remembering and returning reliably to previous promising states. To accomplish this, there are two phases while the first one explore the search space until solved with high performance, while the second phase utilizes phase one as guidance to produce the actual policy. 

#### Phase 1
In the original work, the algorithm is implemented and tested on classic Atari games "Montezumaâ€™s Revenge" and "Pitfall". Therefore, it is necessary to record promising game states (frames), as so-called cells by the authors. As it is more computationally efficient, a good cell representation should reduce dimensionality of frames into meaningful low dimensional space. Then, the algorithm keeps an archive of cells and chooses a cell, from which to explore next. Such a cell can be selected uniformly, or by some designed heuristic. In the original work, as Atari games are naturally resettable and deterministic, the training process is solely deterministic. Additionally, the exploration from individual cells is entirely random with no involvement of neural network or designed policy. 

#### Phase 2
As phase 1 is entirely trained with deterministic strategy, the produced policy is not robust in a stochastic environment. Therefore, in phase 2, learning from demonstration (LfD) algorithms are employed to use phase 1 policy as guidance and produce a final policy which is also robust in stochastic situations. 

## Contributions
To experiment on the possible modules of the original work and possibly improve the performance, we focus on the following modules:
* Cell representation: instead of naively downscaling by averaging, it is possible to utilize some pretrained computer vision models.
* Cell heuristics: instead of random choice, cells can be chosen by better heuristics which give better guidance on more promising states.
* Stochastic training: it is possible to include stochastic elements in the training process and derive a policy robust enough to save the efforts of phase 2 entirely.
* Cell exploration strategy: instead of random exploration from a chosen cell, a devised policy can be formulated to move to more promising states.
* LfD choices: it is possible to experiment on other LfD models used in phase 2 and see if we can achieve better results.

### Cell representation

### Cell heuristics

### Stochastic training

### Cell exploration strategy

### LfD choices

## Implementation

## Results

## Conclusion


## Reference
[1] Ecoffet, Adrien, et al. "Go-explore: a new approach for hard-exploration problems." arXiv preprint arXiv:1901.10995 (2019)

---








