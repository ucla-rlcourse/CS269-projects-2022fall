---
layout: post
comments: true
title: Exploring Generalizability in Autonomous Driving Simulation
author: Tanmay Sanjay Hukkeri, Vivek Arora (Team 09)
date: 2022-10-19
---


> One of the most significant challenges in artificial intelligence is that of autonomous vehicle control. The problem is usually modeled as a series of sequential choices accompanied by feedback from the environment, with the goal of the “agent” being to gradually improve control policies through experience and system feedback via reinforcement learning. A key facet of testing and improving autonomous driving is the use of simulators. Simulators provide an environment to experiment, train and test RL algorithms on benchmarks of success, generalization and safety, without requiring physical resources and environments. This project seeks to take view of this domain, and further contribute to the search for generalizability in autonomous vehicle control.


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Problem Statement
The goals of this project can thus be detailed as follows:
- To set up a working autonomous vehicle simulator and successfully initiate training RL based agents.
- To explore the impact of learning under several different algorithms, such as A2C/A3C[1] and ACKTR[2], and develop an agent that can smoothly navigate the environment.
- To extend the above learning towards generalizability along several potential directions, some of which include:-
    - Generalizability towards differing map layouts and obstacles.
    - Generalizability towards differing weather conditions.
    - Generalizability of the proposed algorithm and associated parameters across simulator environments.

<br/>

## Proposed Environments

Our initial investigation currently involves identifying the ideal environment best suited towards our experimentation. Key considerations include:
- Cross-platform compatibility :  The environment should be able to operate on OSX (MacOS with M1/M2 chip) systems / Google Colab without any limitations.
- Lightweight: The environment should provide for observations that allow for rapid training and experimentation , given the project timelines.
- Customizable : Given the extended goal towards exploring generalisability, the chosen environment should allow for some customization of maps, such as layouts, obstacles etc. 

Based on the above considerations,we conducted an initial investigation of the available environments, and shortlisted the below as potential candidates for the project :

### MetaDrive 
MetaDrive[3] is a driving simulator with the following key features:
- Compositional: It supports generating infinite scenes with various road maps and traffic settings for the research of generalizable RL.
- Lightweight: It is easy to install and run. It can run up to 300 FPS on a standard PC.
- Realistic: Accurate physics simulation and multiple sensory input including Lidar, RGB images, top-down semantic map and first-person view images.

### Gym-Duckietown
Gym-Duckietown[4] is a simulator for the Duckietown environment, which allows the training of (Duckiebot) agents in maps customisable with intersections, obstacles and pedestrians.
The environment is lightweight, and offers easy customization, as well as access to scripts for both Reinforcement Learning and Imitation Learning.


### SMARTS
SMARTS (Scalable Multi-Agent RL Training School)[5] is a dedicated simulation platform which supports the training, accumulation, and use of diverse behavior models of road users.

<br/>

## Current Progress

At this stage, we are working towards finalizing the environment, by attempting to setup and test out the aforementioned three choices on Mac OSX / Google Colab:

### MetaDrive 
We were able to setup Metadrive on Google Colab without the 3D rendering functionality and run evaluation on both Generalization and Safe Drive Environments using their pretrained expert PPO policy. We were able to visualize the sample episodes as follows:

![MetaDriveSimulate]({{ '/assets/images/team09/metadrive_demo.gif' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. Metadrive sample visualisation*.

### Gym-Duckietown
We have been able to successfully set up a local build of gym-duckietown, test out the simulator as well as run a sample training script of 200000 steps. 
On average, it takes about 4hrs to train 100000 steps using the Mac OSX GPU.
The below samples demonstrate a few sample runs on the simulator with the aforementioned incompletely trained model.

![PartialSuccess]({{ '/assets/images/team09/duck_partial_success.gif' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 2a. Partial Success Case*.


![Failure]({{ '/assets/images/team09/duck_fail.gif' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 2b. Failure Case*.


### SMARTS
We are currently troubleshooting issues on setting up the simulator on an OSX machine, with the major issues being around managing installation dependencies. There is a version mismatch on a sumo-eclispe library which is causing an error. 

## Proposed Timeline

The rough timeline for our project is as follows:
- Week 4 - Finalize the environment based on key considerations of hardware compatibility, training time and customizability
- Week 5,6,7 - Train various RL agents across different algorithms
- Week 8,9 - Explore Generalizability and (if time permits) Safety
- Week 10,11 - Presentation, Code Cleanup and Report


## References
[1]Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. &amp; Kavukcuoglu, K.. (2016). Asynchronous Methods for Deep Reinforcement Learning. <i>Proceedings of The 33rd International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 48:1928-1937 Available from https://proceedings.mlr.press/v48/mniha16.html.  

[2]Wu, Y., Mansimov, E., Grosse, R.B., Liao, S., & Ba, J. (2017). Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation. NIPS.  

[3]Li, Quanyi & Peng, Zhenghao & Xue, Zhenghai & Zhang, Qihang & Zhou, Bolei. (2021). MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning.  

[4]Maxime Chevalier-Boisvert, Florian Golemo, Yanjun Cao, Bhairav Mehta, and Liam
Paull. Duckietown environments for openai gym. https://github.com/duckietown/
gym-duckietown, 2018.  

[5]Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang,
Montgomery Alban, Iman Fadakar, Zheng Chen, et al. Smarts: Scalable multi-agent rein-
forcement learning training school for autonomous driving. arXiv preprint arXiv:2010.09776,
2020
