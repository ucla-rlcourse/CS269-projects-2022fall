---
layout: post
comments: true
title: Exploring Generalizability in Autonomous Driving Simulation using MetaDrive
author: Tanmay Sanjay Hukkeri, Vivek Arora (Team 09)
date: 2022-10-19
---


> One of the most significant challenges in artificial intelligence is that of autonomous vehicle control. The problem is usually modeled as a continuous stream of actions accompanied by feedback from the environment, with the goal of the “agent” being to gradually improve control policies through experience and system feedback via reinforcement learning. A key facet of testing and improving autonomous driving is the use of simulators. Simulators provide an environment to experiment, train and test RL algorithms on benchmarks of success, generalization and safety, without requiring physical resources and environments. This project seeks to take view of this domain, and further contribute to the search for generalizability in autonomous vehicle control.


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Problem Statement
The goals of this project can thus be detailed as follows:
- To set up a working autonomous vehicle simulator and successfully initiate training RL based agents.
- To explore the impact of learning under several different algorithms, such as DDPG [1], TD3[2] and PPO[3] and develop an agent that can smoothly navigate the environment.
- To extend the above learning towards generalizability along several potential directions, some of which include:-
    - Generalizability towards differing map layouts and obstacles.
    - Generalizability of the proposed algorithm and associated parameters across simulator environments.
<br/>

## Environment

Our initial investigation involved identifying the ideal environment best suited towards our experimentation. Key considerations include:
- **Cross-platform compatibility** :  The environment should be able to operate on OSX (MacOS with M1/M2 chip) systems / Google Colab without any limitations.
- **Lightweight**: The environment should provide for observations that allow for rapid training and experimentation , given the project timelines.
- **Customizable** : Given the extended goal towards exploring generalisability, the chosen environment should allow for some customization of maps, such as layouts, obstacles etc. 

Based on the above considerations, we narrowed down on choosing the MetaDrive[4] environment for our experiments and project. MetaDrive is a driving simulator with the following key features:
- **Compositional**: It supports generating infinite scenes with various road maps and traffic settings for the research of generalizable RL.
- **Lightweight**: It is easy to install and run. It can run up to 300 FPS on a standard PC.
- **Realistic**: Accurate physics simulation and multiple sensory input including Lidar, RGB images, top-down semantic map and first-person view images.

We make use of the top-down view for our experiments.
<br/>

## Current Progress

### Maps and Environment

In order to work towards developing generalizability , we plan to test out different config parameters under the <i>MetaDriveEnv</i> construct to generate different kinds of layouts and maps. Some of the key parameters that we plan to focus experiments on include:
- **lane_width, lane_num, random_lane_width, random_lane_num** : To vary the number of layouts and allow for randomness
- Map config properties such as **block_num, block_sequence and block_type**
- **traffic_density**

Some sample map layouts that can be consequently generated are shown in the figure below 

|    :----:                                                                                                    |    :----:                                                                                                |                                                       
|![Map1]({{ '/assets/images/team09/map1.png' | relative_url }}){: style="width: 500px; max-width: 100%;"}      | ![Map2]({{ '/assets/images/team09/map2.png' | relative_url }}){: style="width: 500px; max-width: 100%;"} |  

*Fig 1. Sample MetaDrive Map Layouts*.

### Experimentation

As a baseline experiment, we create a training environment similar to "MetaDrive-Tut-Hard-v0", with :
- ENVIRONMENT_NUM = 20
- START_SEED = 1000

We train agents using different algorithms, namely : DQN[5], PGD (with baseline)[6], PPO, DDPG and TD3. Each algorithm is trained for a total of 1000000 timesteps (except PPO which we train for 1500000 timesteps) , with differing training hyperparameters based on the algorithm involved.

The figure below provides a comparitive evaluation of each of the algorithms on the same episode. Note that the vehicle in green represents the trained agent. 

|    :----:                                                                                                                |    :----:                                                                                                                       |                                                       
| **DQN** ![DQN]({{ '/assets/images/team09/dqn_final.gif' | relative_url }}){: style="width: 500px; max-width: 100%;"}      | **PGD(with baseline)** ![PGD]({{ '/assets/images/team09/pgd_final.gif' | relative_url }}){: style="width: 500px; max-width: 100%;"} |  
| **DDPG** ![DDPG]({{ '/assets/images/team09/ddpg_final.gif' | relative_url }}){: style="width: 500px; max-width: 100%;"}   | **TD3**    ![TD3]({{ '/assets/images/team09/td3_final.gif' | relative_url }}){: style="width: 500px; max-width: 100%;"}         |  
| **PPO** ![PPO]({{ '/assets/images/team09/ppo_final.gif' | relative_url }}){: style="width: 500px; max-width: 100%;"} |                                                                                                                     

*Fig 2. Baseline implementation of the various algorithms*.

As we can see, we notice promising results from **DDPG,TD3 and PPO**. We then further evaluate the above trained models on a different episode generated from a modified map configuaration:
- lane_num = 4 (default 3)
- traffic_density = 0.3 (default 0.1)

The figure below demonstraates the results of the above experiments on this unseen map setting.  

|    :----:                                                                                                                |    :----:                                                                                                                       |                                                       
| **DQN** ![DQN_Gen]({{ '/assets/images/team09/dqn_final_gen.gif' | relative_url }}){: style="width: 500px; max-width: 100%;"}      | **PGD(with baseline)** ![PGD_Gen]({{ '/assets/images/team09/pgd_final_gen.gif' | relative_url }}){: style="width: 500px; max-width: 100%;"} |  
| **DDPG** ![DDPG_Gen]({{ '/assets/images/team09/ddpg_final_gen.gif' | relative_url }}){: style="width: 500px; max-width: 100%;"}   | **TD3**    ![TD3_Gen]({{ '/assets/images/team09/td3_final_gen.gif' | relative_url }}){: style="width: 500px; max-width: 100%;"}         |  
| **PPO** ![PPO_Gen]({{ '/assets/images/team09/ppo_final_gen.gif' | relative_url }}){: style="width: 500px; max-width: 100%;"} |     

*Fig 3. Baseline implementation of the various algorithms on new map configuration*.

We also document the baseline quantative results of training the above algorithms in the table below

![Table1]({{ '/assets/images/team09/table_1.png' | relative_url }}){: style="max-width: 100%;"}
*Table 1. Initial Quantitative Analysis*.

### Initial Analysis

From the above baseline experiments , we can observe that:
 - More complex algorithms such as DDPG, TD3 and PPO outperform simpler methods like DQN and PGD on the complex environment.
 - For the given training settings, TD3 and DDPG have almost similar returns given an episode, however, the path taken by the TD3 policy based vehicle seems more stable and switches lanes lesser than that taken by DDPG.
 - With PGD, the policy seems to be generalising the vehicle to move in a straight path.
<br/>

## Proposed Work and Timeline

Based on the baseline experiments conducted in the previous section, we can observe that <insert algorithms here> seem the promising in working well in complex environments. We plan to experiment along different lines to identify and develop a model that easily generalises towards more complex environments, through potential approaches such as:
- Modifying the underlying neural network architecture and hyperparameters for the above algorithmic approaches.
- Training across different maps, while identifying the key variables that can drive towards a generalizable model.
- Modifying/Tweaking the reward functions to tune the algorithmic output

If time permits, we also plan to explore whether the algorithm best identified in this environment can translate well to other settings such as:
- Different observation view in MetaDrive
- The Gym-DuckieTown environment[7]

The updated timeline for our project is as follows:
- ~~Week 4 - Finalize the environment based on key considerations of hardware compatibility, training time and customizability~~ **Completed**
- ~~Week 5,6,7 - Train various RL agents across different algorithms~~ **Completed**
- Week 8,9 - Try out SAC algorithm[8], Explore Generalizability
- Week 10,11 - Presentation, Code Cleanup and Report
<br/>

## References
[1] Lillicrap, Timothy P. et al. “Continuous control with deep reinforcement learning.” CoRR abs/1509.02971 (2016): n. pag.  

[2] Fujimoto, Scott et al. “Addressing Function Approximation Error in Actor-Critic Methods.” ArXiv abs/1802.09477 (2018): n. pag.  

[3] Schulman, J., Wolski, F., Dhariwal, P., Radford, A. & Klimov, O. (2017). Proximal Policy Optimization Algorithms.. CoRR, abs/1707.06347.  

[4] Li, Quanyi & Peng, Zhenghao & Xue, Zhenghai & Zhang, Qihang & Zhou, Bolei. (2021). MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning.  

[5] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D. & Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning.  

[6] Sutton, McAllester, Singh, Mansour (1999). Policy gradient methods for reinforcement learning with function
approximation: actor-critic algorithms with value function approximation 

[7]Maxime Chevalier-Boisvert, Florian Golemo, Yanjun Cao, Bhairav Mehta, and Liam Paull. Duckietown environments for openai gym. https://github.com/duckietown/
gym-duckietown, 2018.  

[8]Haarnoja, Tuomas et al. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” ICML (2018).
