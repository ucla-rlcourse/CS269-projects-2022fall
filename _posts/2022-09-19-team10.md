---
layout: post
comments: true
title: The exploration of policy dissection
author: Ruikang Wu and Mengyuan Zhang (Team 10)
date: 2022-10-19
---

> In many complex tasks, RL-trained policy may not solve the tasks efficiently. The training process may cost people too much time. Policy dissection is a frequency-based method, which can convert RL-trained policy into target-conditioned policy. For this method, we can interacte with the kinematic behavior and the learned neural controller. In this project, we want to explore the implementation of the policy dissection. We will figure out how we can use the policy deissection to solve some potential problems in the real world.


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
This article will refer to the [source code](https://github.com/metadriverse/policydissect) .

First of all, we want to explore the relationship between kinematic behavior and the neural activities. In this experiment, we want to figure out the patterns behind some certain behaviors. 

Second, we want to train bipedal robots, Cassie, in [IsaacGym](https://github.com/NVIDIA-Omniverse/IsaacGymEnvs), based on the patterns that we figured out. The robots will be trained to have some complex behaviors such as crouch, turn left, turn right, back flip, tiptoe, jump etc. 

Third, we want to explore whether the bipedal robots can handle more complex situtions within the human instruction.

Fourth, we want to explore whether we can improve the algoirthm. For improving the algorithm, we may potentially compare the policy based, value-based and model based methods, and choose the best performance methods.

Finally, we will try to figure out whther we can implement the policy dissection to the other complex tasks in the real world. We are still working on finding the real world problems. 

## Policy Dissection
- What is the Policy Dissection?

    Previous reinforcement learning methods have attempted to design goals-conditioned policies that can achieve human goals, but at the cost of redesigning reward functions and training paradigms. Hence the Policy Dissection, an approach inspired by neuroscience methods based on the primate motor cortex. Using this approach, manually controllable policies can be derived in trained reinforcement learning agents. Experiments show that under the blessing of Policy Dissection, the motion robot exhibits a variety of controllable motor skills.

- Four main steps for Policy Dissection

    **1. Monitoring Neural Activity and Kinematics:** The policy dissection expands the trained policy and records the tracked neural activities and kinematic attributes, such as velocity and yaw. It will recorde the neural activities and kinematic attributes for further anaylsis.

    **2. Associating Units with Kinematic Attributes:** According to the records of neural activities and kinematic attributes, the same kinematic patterns will appear with different frequencies. So for a kinematic patterns, the unit with the smallest frequency discrepancy is the motor primitive. 

    **3. Building Stimulation-evoked Map:** Behavior can be described by changing a subset of kinematic attributes, or by activating a corresponding set of motor primitives. These movements are associated with certain behaviors to generate building blocks that include stimulation-evoked map.

    **4. Steering Agent via Stimulation-evoked Map:** 

- Workflow of Policy Dissection

    ![workflow]({{ '/assets/images/team10/algorithm.png'| relative_url}})
    {: style="width: 400px; max-width: 100%;"}
    *Fig 1. workflow of Policy Dissection* [1].

## Installation
### Basic Installation:
We completed the basic installation according to the [
policy dissect's](https://github.com/metadriverse/policydissect.git) instructions. It contains some basic packages we need.

### Environment Installation:
We also installed some supported environments for testing the policy dissection method.
- [MetaDrive](https://github.com/metadriverse/metadrive.git)

    ![metadrive]({{ '/assets/images/team10/metadrive.png'| relative_url}}){: style="width: 400px; max-width: 100%;"}

- [Pybullet-Quadrupedal Robot](https://github.com/Mehooz/vision4leg.git)

    ![pybullet]({{ '/assets/images/team10/pybullet.png'| relative_url}}){: style="width: 400px; max-width: 100%;"}

- [Gym-BipedalWalker](https://github.com/openai/gym.git)

    ![bipedal]({{ '/assets/images/team10/bipedal.png'| relative_url}}){: style="width: 400px; max-width: 100%;"}

## Reference
[1] Q. Li, Z. Peng, H. Wu, L. Feng, and B. Zhou. Human-AI shared control via policy dissection. *arXiv preprint arXiv:2206.00152*, 2022. 

[2] D. Bau, J.-Y. Zhu, H. Strobelt, A. Lapedriza, B. Zhou, and A. Torralba. Understanding the role of individual
units in a deep neural network. *Proceedings of the National Academy of Sciences*, 117(48):30071–30078, 2020.

[3] S. Guo, R. Zhang, B. Liu, Y. Zhu, D. Ballard, M. Hayhoe, and P. Stone. Machine versus human attention
in deep reinforcement learning tasks. *Advances in Neural Information Processing Systems*, 34, 2021.

[4] B. Zhou, D. Bau, A. Oliva, and A. Torralba. Interpreting deep visual representations via network dissection. *IEEE transactions on pattern analysis and machine intelligence*, 41(9):2131–2145, 2018.